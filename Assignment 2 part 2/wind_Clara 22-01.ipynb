{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SOME PRELIMINARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Clara\\\\Documents'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "# For Panda dataframes. A dataframe is a matrix-like structure, \n",
    "# similar to R dataframes  \n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['energy',\n",
       " 'steps',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'p54.162.1',\n",
       " 'p54.162.2',\n",
       " 'p54.162.3',\n",
       " 'p54.162.4',\n",
       " 'p54.162.5',\n",
       " 'p54.162.6',\n",
       " 'p54.162.7',\n",
       " 'p54.162.8',\n",
       " 'p54.162.9',\n",
       " 'p54.162.10',\n",
       " 'p54.162.11',\n",
       " 'p54.162.12',\n",
       " 'p54.162.13',\n",
       " 'p54.162.14',\n",
       " 'p54.162.15',\n",
       " 'p54.162.16',\n",
       " 'p54.162.17',\n",
       " 'p54.162.18',\n",
       " 'p54.162.19',\n",
       " 'p54.162.20',\n",
       " 'p54.162.21',\n",
       " 'p54.162.22',\n",
       " 'p54.162.23',\n",
       " 'p54.162.24',\n",
       " 'p54.162.25',\n",
       " 'p55.162.1',\n",
       " 'p55.162.2',\n",
       " 'p55.162.3',\n",
       " 'p55.162.4',\n",
       " 'p55.162.5',\n",
       " 'p55.162.6',\n",
       " 'p55.162.7',\n",
       " 'p55.162.8',\n",
       " 'p55.162.9',\n",
       " 'p55.162.10',\n",
       " 'p55.162.11',\n",
       " 'p55.162.12',\n",
       " 'p55.162.13',\n",
       " 'p55.162.14',\n",
       " 'p55.162.15',\n",
       " 'p55.162.16',\n",
       " 'p55.162.17',\n",
       " 'p55.162.18',\n",
       " 'p55.162.19',\n",
       " 'p55.162.20',\n",
       " 'p55.162.21',\n",
       " 'p55.162.22',\n",
       " 'p55.162.23',\n",
       " 'p55.162.24',\n",
       " 'p55.162.25',\n",
       " 'cape.1',\n",
       " 'cape.2',\n",
       " 'cape.3',\n",
       " 'cape.4',\n",
       " 'cape.5',\n",
       " 'cape.6',\n",
       " 'cape.7',\n",
       " 'cape.8',\n",
       " 'cape.9',\n",
       " 'cape.10',\n",
       " 'cape.11',\n",
       " 'cape.12',\n",
       " 'cape.13',\n",
       " 'cape.14',\n",
       " 'cape.15',\n",
       " 'cape.16',\n",
       " 'cape.17',\n",
       " 'cape.18',\n",
       " 'cape.19',\n",
       " 'cape.20',\n",
       " 'cape.21',\n",
       " 'cape.22',\n",
       " 'cape.23',\n",
       " 'cape.24',\n",
       " 'cape.25',\n",
       " 'p59.162.1',\n",
       " 'p59.162.2',\n",
       " 'p59.162.3',\n",
       " 'p59.162.4',\n",
       " 'p59.162.5',\n",
       " 'p59.162.6',\n",
       " 'p59.162.7',\n",
       " 'p59.162.8',\n",
       " 'p59.162.9',\n",
       " 'p59.162.10',\n",
       " 'p59.162.11',\n",
       " 'p59.162.12',\n",
       " 'p59.162.13',\n",
       " 'p59.162.14',\n",
       " 'p59.162.15',\n",
       " 'p59.162.16',\n",
       " 'p59.162.17',\n",
       " 'p59.162.18',\n",
       " 'p59.162.19',\n",
       " 'p59.162.20',\n",
       " 'p59.162.21',\n",
       " 'p59.162.22',\n",
       " 'p59.162.23',\n",
       " 'p59.162.24',\n",
       " 'p59.162.25',\n",
       " 'lai_lv.1',\n",
       " 'lai_lv.2',\n",
       " 'lai_lv.3',\n",
       " 'lai_lv.4',\n",
       " 'lai_lv.5',\n",
       " 'lai_lv.6',\n",
       " 'lai_lv.7',\n",
       " 'lai_lv.8',\n",
       " 'lai_lv.9',\n",
       " 'lai_lv.10',\n",
       " 'lai_lv.11',\n",
       " 'lai_lv.12',\n",
       " 'lai_lv.13',\n",
       " 'lai_lv.14',\n",
       " 'lai_lv.15',\n",
       " 'lai_lv.16',\n",
       " 'lai_lv.17',\n",
       " 'lai_lv.18',\n",
       " 'lai_lv.19',\n",
       " 'lai_lv.20',\n",
       " 'lai_lv.21',\n",
       " 'lai_lv.22',\n",
       " 'lai_lv.23',\n",
       " 'lai_lv.24',\n",
       " 'lai_lv.25',\n",
       " 'lai_hv.1',\n",
       " 'lai_hv.2',\n",
       " 'lai_hv.3',\n",
       " 'lai_hv.4',\n",
       " 'lai_hv.5',\n",
       " 'lai_hv.6',\n",
       " 'lai_hv.7',\n",
       " 'lai_hv.8',\n",
       " 'lai_hv.9',\n",
       " 'lai_hv.10',\n",
       " 'lai_hv.11',\n",
       " 'lai_hv.12',\n",
       " 'lai_hv.13',\n",
       " 'lai_hv.14',\n",
       " 'lai_hv.15',\n",
       " 'lai_hv.16',\n",
       " 'lai_hv.17',\n",
       " 'lai_hv.18',\n",
       " 'lai_hv.19',\n",
       " 'lai_hv.20',\n",
       " 'lai_hv.21',\n",
       " 'lai_hv.22',\n",
       " 'lai_hv.23',\n",
       " 'lai_hv.24',\n",
       " 'lai_hv.25',\n",
       " 'u10n.1',\n",
       " 'u10n.2',\n",
       " 'u10n.3',\n",
       " 'u10n.4',\n",
       " 'u10n.5',\n",
       " 'u10n.6',\n",
       " 'u10n.7',\n",
       " 'u10n.8',\n",
       " 'u10n.9',\n",
       " 'u10n.10',\n",
       " 'u10n.11',\n",
       " 'u10n.12',\n",
       " 'u10n.13',\n",
       " 'u10n.14',\n",
       " 'u10n.15',\n",
       " 'u10n.16',\n",
       " 'u10n.17',\n",
       " 'u10n.18',\n",
       " 'u10n.19',\n",
       " 'u10n.20',\n",
       " 'u10n.21',\n",
       " 'u10n.22',\n",
       " 'u10n.23',\n",
       " 'u10n.24',\n",
       " 'u10n.25',\n",
       " 'v10n.1',\n",
       " 'v10n.2',\n",
       " 'v10n.3',\n",
       " 'v10n.4',\n",
       " 'v10n.5',\n",
       " 'v10n.6',\n",
       " 'v10n.7',\n",
       " 'v10n.8',\n",
       " 'v10n.9',\n",
       " 'v10n.10',\n",
       " 'v10n.11',\n",
       " 'v10n.12',\n",
       " 'v10n.13',\n",
       " 'v10n.14',\n",
       " 'v10n.15',\n",
       " 'v10n.16',\n",
       " 'v10n.17',\n",
       " 'v10n.18',\n",
       " 'v10n.19',\n",
       " 'v10n.20',\n",
       " 'v10n.21',\n",
       " 'v10n.22',\n",
       " 'v10n.23',\n",
       " 'v10n.24',\n",
       " 'v10n.25',\n",
       " 'sp.1',\n",
       " 'sp.2',\n",
       " 'sp.3',\n",
       " 'sp.4',\n",
       " 'sp.5',\n",
       " 'sp.6',\n",
       " 'sp.7',\n",
       " 'sp.8',\n",
       " 'sp.9',\n",
       " 'sp.10',\n",
       " 'sp.11',\n",
       " 'sp.12',\n",
       " 'sp.13',\n",
       " 'sp.14',\n",
       " 'sp.15',\n",
       " 'sp.16',\n",
       " 'sp.17',\n",
       " 'sp.18',\n",
       " 'sp.19',\n",
       " 'sp.20',\n",
       " 'sp.21',\n",
       " 'sp.22',\n",
       " 'sp.23',\n",
       " 'sp.24',\n",
       " 'sp.25',\n",
       " 'stl1.1',\n",
       " 'stl1.2',\n",
       " 'stl1.3',\n",
       " 'stl1.4',\n",
       " 'stl1.5',\n",
       " 'stl1.6',\n",
       " 'stl1.7',\n",
       " 'stl1.8',\n",
       " 'stl1.9',\n",
       " 'stl1.10',\n",
       " 'stl1.11',\n",
       " 'stl1.12',\n",
       " 'stl1.13',\n",
       " 'stl1.14',\n",
       " 'stl1.15',\n",
       " 'stl1.16',\n",
       " 'stl1.17',\n",
       " 'stl1.18',\n",
       " 'stl1.19',\n",
       " 'stl1.20',\n",
       " 'stl1.21',\n",
       " 'stl1.22',\n",
       " 'stl1.23',\n",
       " 'stl1.24',\n",
       " 'stl1.25',\n",
       " 'u10.1',\n",
       " 'u10.2',\n",
       " 'u10.3',\n",
       " 'u10.4',\n",
       " 'u10.5',\n",
       " 'u10.6',\n",
       " 'u10.7',\n",
       " 'u10.8',\n",
       " 'u10.9',\n",
       " 'u10.10',\n",
       " 'u10.11',\n",
       " 'u10.12',\n",
       " 'u10.13',\n",
       " 'u10.14',\n",
       " 'u10.15',\n",
       " 'u10.16',\n",
       " 'u10.17',\n",
       " 'u10.18',\n",
       " 'u10.19',\n",
       " 'u10.20',\n",
       " 'u10.21',\n",
       " 'u10.22',\n",
       " 'u10.23',\n",
       " 'u10.24',\n",
       " 'u10.25',\n",
       " 'v10.1',\n",
       " 'v10.2',\n",
       " 'v10.3',\n",
       " 'v10.4',\n",
       " 'v10.5',\n",
       " 'v10.6',\n",
       " 'v10.7',\n",
       " 'v10.8',\n",
       " 'v10.9',\n",
       " 'v10.10',\n",
       " 'v10.11',\n",
       " 'v10.12',\n",
       " 'v10.13',\n",
       " 'v10.14',\n",
       " 'v10.15',\n",
       " 'v10.16',\n",
       " 'v10.17',\n",
       " 'v10.18',\n",
       " 'v10.19',\n",
       " 'v10.20',\n",
       " 'v10.21',\n",
       " 'v10.22',\n",
       " 'v10.23',\n",
       " 'v10.24',\n",
       " 'v10.25',\n",
       " 't2m.1',\n",
       " 't2m.2',\n",
       " 't2m.3',\n",
       " 't2m.4',\n",
       " 't2m.5',\n",
       " 't2m.6',\n",
       " 't2m.7',\n",
       " 't2m.8',\n",
       " 't2m.9',\n",
       " 't2m.10',\n",
       " 't2m.11',\n",
       " 't2m.12',\n",
       " 't2m.13',\n",
       " 't2m.14',\n",
       " 't2m.15',\n",
       " 't2m.16',\n",
       " 't2m.17',\n",
       " 't2m.18',\n",
       " 't2m.19',\n",
       " 't2m.20',\n",
       " 't2m.21',\n",
       " 't2m.22',\n",
       " 't2m.23',\n",
       " 't2m.24',\n",
       " 't2m.25',\n",
       " 'stl2.1',\n",
       " 'stl2.2',\n",
       " 'stl2.3',\n",
       " 'stl2.4',\n",
       " 'stl2.5',\n",
       " 'stl2.6',\n",
       " 'stl2.7',\n",
       " 'stl2.8',\n",
       " 'stl2.9',\n",
       " 'stl2.10',\n",
       " 'stl2.11',\n",
       " 'stl2.12',\n",
       " 'stl2.13',\n",
       " 'stl2.14',\n",
       " 'stl2.15',\n",
       " 'stl2.16',\n",
       " 'stl2.17',\n",
       " 'stl2.18',\n",
       " 'stl2.19',\n",
       " 'stl2.20',\n",
       " 'stl2.21',\n",
       " 'stl2.22',\n",
       " 'stl2.23',\n",
       " 'stl2.24',\n",
       " 'stl2.25',\n",
       " 'stl3.1',\n",
       " 'stl3.2',\n",
       " 'stl3.3',\n",
       " 'stl3.4',\n",
       " 'stl3.5',\n",
       " 'stl3.6',\n",
       " 'stl3.7',\n",
       " 'stl3.8',\n",
       " 'stl3.9',\n",
       " 'stl3.10',\n",
       " 'stl3.11',\n",
       " 'stl3.12',\n",
       " 'stl3.13',\n",
       " 'stl3.14',\n",
       " 'stl3.15',\n",
       " 'stl3.16',\n",
       " 'stl3.17',\n",
       " 'stl3.18',\n",
       " 'stl3.19',\n",
       " 'stl3.20',\n",
       " 'stl3.21',\n",
       " 'stl3.22',\n",
       " 'stl3.23',\n",
       " 'stl3.24',\n",
       " 'stl3.25',\n",
       " 'iews.1',\n",
       " 'iews.2',\n",
       " 'iews.3',\n",
       " 'iews.4',\n",
       " 'iews.5',\n",
       " 'iews.6',\n",
       " 'iews.7',\n",
       " 'iews.8',\n",
       " 'iews.9',\n",
       " 'iews.10',\n",
       " 'iews.11',\n",
       " 'iews.12',\n",
       " 'iews.13',\n",
       " 'iews.14',\n",
       " 'iews.15',\n",
       " 'iews.16',\n",
       " 'iews.17',\n",
       " 'iews.18',\n",
       " 'iews.19',\n",
       " 'iews.20',\n",
       " 'iews.21',\n",
       " 'iews.22',\n",
       " 'iews.23',\n",
       " 'iews.24',\n",
       " 'iews.25',\n",
       " 'inss.1',\n",
       " 'inss.2',\n",
       " 'inss.3',\n",
       " 'inss.4',\n",
       " 'inss.5',\n",
       " 'inss.6',\n",
       " 'inss.7',\n",
       " 'inss.8',\n",
       " 'inss.9',\n",
       " 'inss.10',\n",
       " 'inss.11',\n",
       " 'inss.12',\n",
       " 'inss.13',\n",
       " 'inss.14',\n",
       " 'inss.15',\n",
       " 'inss.16',\n",
       " 'inss.17',\n",
       " 'inss.18',\n",
       " 'inss.19',\n",
       " 'inss.20',\n",
       " 'inss.21',\n",
       " 'inss.22',\n",
       " 'inss.23',\n",
       " 'inss.24',\n",
       " 'inss.25',\n",
       " 'stl4.1',\n",
       " 'stl4.2',\n",
       " 'stl4.3',\n",
       " 'stl4.4',\n",
       " 'stl4.5',\n",
       " 'stl4.6',\n",
       " 'stl4.7',\n",
       " 'stl4.8',\n",
       " 'stl4.9',\n",
       " 'stl4.10',\n",
       " 'stl4.11',\n",
       " 'stl4.12',\n",
       " 'stl4.13',\n",
       " 'stl4.14',\n",
       " 'stl4.15',\n",
       " 'stl4.16',\n",
       " 'stl4.17',\n",
       " 'stl4.18',\n",
       " 'stl4.19',\n",
       " 'stl4.20',\n",
       " 'stl4.21',\n",
       " 'stl4.22',\n",
       " 'stl4.23',\n",
       " 'stl4.24',\n",
       " 'stl4.25',\n",
       " 'fsr.1',\n",
       " 'fsr.2',\n",
       " 'fsr.3',\n",
       " 'fsr.4',\n",
       " 'fsr.5',\n",
       " 'fsr.6',\n",
       " 'fsr.7',\n",
       " 'fsr.8',\n",
       " 'fsr.9',\n",
       " 'fsr.10',\n",
       " 'fsr.11',\n",
       " 'fsr.12',\n",
       " 'fsr.13',\n",
       " 'fsr.14',\n",
       " 'fsr.15',\n",
       " 'fsr.16',\n",
       " 'fsr.17',\n",
       " 'fsr.18',\n",
       " 'fsr.19',\n",
       " 'fsr.20',\n",
       " 'fsr.21',\n",
       " 'fsr.22',\n",
       " 'fsr.23',\n",
       " 'fsr.24',\n",
       " 'fsr.25',\n",
       " 'flsr.1',\n",
       " 'flsr.2',\n",
       " 'flsr.3',\n",
       " 'flsr.4',\n",
       " 'flsr.5',\n",
       " 'flsr.6',\n",
       " 'flsr.7',\n",
       " 'flsr.8',\n",
       " 'flsr.9',\n",
       " 'flsr.10',\n",
       " 'flsr.11',\n",
       " 'flsr.12',\n",
       " 'flsr.13',\n",
       " 'flsr.14',\n",
       " 'flsr.15',\n",
       " 'flsr.16',\n",
       " 'flsr.17',\n",
       " 'flsr.18',\n",
       " 'flsr.19',\n",
       " 'flsr.20',\n",
       " 'flsr.21',\n",
       " 'flsr.22',\n",
       " 'flsr.23',\n",
       " 'flsr.24',\n",
       " 'flsr.25',\n",
       " 'u100.1',\n",
       " 'u100.2',\n",
       " 'u100.3',\n",
       " 'u100.4',\n",
       " 'u100.5',\n",
       " 'u100.6',\n",
       " 'u100.7',\n",
       " 'u100.8',\n",
       " 'u100.9',\n",
       " 'u100.10',\n",
       " 'u100.11',\n",
       " 'u100.12',\n",
       " 'u100.13',\n",
       " 'u100.14',\n",
       " 'u100.15',\n",
       " 'u100.16',\n",
       " 'u100.17',\n",
       " 'u100.18',\n",
       " 'u100.19',\n",
       " 'u100.20',\n",
       " 'u100.21',\n",
       " 'u100.22',\n",
       " 'u100.23',\n",
       " 'u100.24',\n",
       " 'u100.25',\n",
       " 'v100.1',\n",
       " 'v100.2',\n",
       " 'v100.3',\n",
       " 'v100.4',\n",
       " 'v100.5',\n",
       " 'v100.6',\n",
       " 'v100.7',\n",
       " 'v100.8',\n",
       " 'v100.9',\n",
       " 'v100.10',\n",
       " 'v100.11',\n",
       " 'v100.12',\n",
       " 'v100.13',\n",
       " 'v100.14',\n",
       " 'v100.15',\n",
       " 'v100.16',\n",
       " 'v100.17',\n",
       " 'v100.18',\n",
       " 'v100.19',\n",
       " 'v100.20',\n",
       " 'v100.21',\n",
       " 'v100.22',\n",
       " 'v100.23',\n",
       " 'v100.24',\n",
       " 'v100.25']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including \n",
    "# the outcome to be predicted)\n",
    "print(data.shape)\n",
    "data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below, data is going to be separated in train, validation, and test. Given that the use of Pandas dataframes is quite advanced, and doing this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indicesTrain = (np.where(data.year<=2006))[0]\n",
    "indicesVal = (np.where((data.year==2007) | (data.year==2008)))[0]\n",
    "indicesTest = (np.where(data.year>=2009))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware!, **indicesTrain** does not contain the training data, but the *indices* of the training data. For instance, the following cell means that training data is made of instance number 0, instance number 1, ..., up to instance number 2527. This will be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2525, 2526, 2527], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicesTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to transform **data**, which is a Pandas dataframe, to **ava**, which is a NumPy matrix. The reason is that Scikit-learn uses NumPy matrices, not Panda dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ava = data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **ava** is going to be decomposed into inputs **X** and outputs **y**. And then, into training, validation, and test. For instance, **Xava** and **yava** contain the input attributes, and the output attribute (**energy**) of the whole dataset. Please, ask yourself why the inputs use \"6:\" and the output use \"0\". **Xtrain** and **ytrain** are the same, but for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299L, 550L)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xava = ava[:,6:]; yava = ava[:,0]\n",
    "Xtrain = ava[indicesTrain,6:]; ytrain = ava[indicesTrain,0]\n",
    "Xval = ava[indicesVal,6:]; yval = ava[indicesVal,0]\n",
    "Xtest = ava[indicesTest,6:][:,6:]; ytest = ava[indicesTest,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following cell defines function **mae** (Mean Absolute Error), that we will use later to measure the accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mae(yval_pred, yval):\n",
    "  val_mae = metrics.mean_absolute_error(yval_pred, yval)\n",
    "  return(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains KNN with (Xtrain, ytrain) and evaluates it with (Xval, yval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80 ms\n",
      "MAE for KNN with K=5 is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "np.random.seed(0)\n",
    "%time _ = knn.fit(Xtrain, ytrain)\n",
    "yval_pred = knn.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with K=5 is {}\".format(mae(yval_pred, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in sklearn.neighbors:\n",
      "\n",
      "sklearn.neighbors.KNeighborsRegressor = class KNeighborsRegressor(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.SupervisedFloatMixin, sklearn.base.RegressorMixin)\n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`k_neighbors` queries.\n",
      " |  \n",
      " |  weights : str or callable\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDtree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  metric : string or DistanceMetric object (default='minkowski')\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of the DistanceMetric class for a\n",
      " |      list of available metrics.\n",
      " |  \n",
      " |  p : integer, optional (default = 2)\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [ 0.5]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  NearestNeighbors\n",
      " |  RadiusNeighborsRegressor\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.SupervisedFloatMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of int, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Target values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.SupervisedFloatMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values, array of float values, shape = [n_samples]\n",
      " |           or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In case you need help for KNN\n",
    "help('sklearn.neighbors.KNeighborsRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell, does hyper-parameter tuning for parameter K (n_neighbors), from 1 to 4 by 1. Please, notice that with **partitions = [(indicesTrain, indicesVal)]** we are telling **gridSearch** to use the training dataset for training the different models with the different parameters, and the validation dataset for testing. Notice that this is different to other notebooks, where crossvalidation was used for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_neighbors': range(1,4,1)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "clf = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "%time _ = clf.fit(Xava,yava)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the best K parameter and the MAE of the final model built with the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: {'n_neighbors': 3} and MAE for best K: 503.711691044\n"
     ]
    }
   ],
   "source": [
    "print \"Best K: {} and MAE for best K: {}\".format(clf.best_params_, -clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HOW LONG DOES IT TAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to have some estimation of how long your machine learning algorithm is going to take. In the next two cells, try to estimate how many seconds KNN (with K=3) does it take, with only **100 instances**. With 6000 instances, it will take approximately 60 times that number. You can use **%time** for timing, as in previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR TIMING KNN>\n",
    "\n",
    "n_neighbors = 3\n",
    "Xtrain_100 = ava[:100,6:]; ytrain_100 = ava[:100,0]\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = knn.fit(Xtrain_100, ytrain_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do the same for Decision trees with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44 ms\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR TIMING Decision Trees>\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "reg = tree.DecisionTreeRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = reg.fit(Xtrain_100, ytrain_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 88 ms\n",
      "MAE for KNN with K=5 (default) is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR KNN>\n",
    "\n",
    "knn_def = neighbors.KNeighborsRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = knn_def.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "yval_pred_knn_def = knn_def.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with K={} (default) is {}\".format(knn_def.n_neighbors, mae(yval_pred_knn_def, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for KNN. Can you improve results? Note: if **gridSearch** takes too long, you can use **Randomized Search** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 38 candidates, totalling 38 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   23.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal combination of parameters obtained with Grid Search is: {'n_neighbors': 17, 'weights': 'distance'} with 469.147029647 MAE.\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR KNN>\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': range(1,20,1),\n",
    "    'weights' : ['uniform', 'distance']   \n",
    "}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "knn_grid = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid_knn,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "knn_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal combination of parameters obtained with Grid Search is: \" + str(knn_grid.best_params_) + \" with \" + str(-knn_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do improve the results, because the Mean Absolute Error is reduced.\n",
    "* We obtained a MAE = 486.911414935 with K=5 (default).\n",
    "* We obtain a MAE = 469.147029647 with K=17 (optimal parameter obtained with Grid Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree for regression with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.2 s\n",
      "MAE for decision tree regressor with max_depth=None and min_samples_leaf=1 (default) is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR DECISION TREES>\n",
    "\n",
    "reg_def = tree.DecisionTreeRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = reg_def.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_reg_def = reg_def.predict(Xval)\n",
    "\n",
    "print \"MAE for decision tree regressor with max_depth={} and min_samples_leaf={} (default) is {}\".format(reg_def.max_depth, reg_def.min_samples_leaf, mae(yval_pred_knn_def, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Decision trees. Can you improve results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 81 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   29.5s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of max_depth and min_samples_leaf obtained with Grid Search is: {'max_depth': 5, 'min_samples_leaf': 6} with 303.111990619 MAE.\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR DECISION TREES>\n",
    "\n",
    "param_grid_reg = {\n",
    "    'max_depth': range(1,10,1),\n",
    "    'min_samples_leaf': range(1,10,1) \n",
    "}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "reg_grid = GridSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid_reg,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "reg_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of max_depth and min_samples_leaf obtained with Grid Search is: \" + str(reg_grid.best_params_) + \" with \" + str(-reg_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest (RF) with default parameters. A RF is an ensemble technique based on Decision Trees, but instead of training just a single decision tree, it trains many of them and then computes the average of the outputs. Please, bear in mind that a RF with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.6 s\n",
      "MAE for Random Forest with n_estimators=10 (default) is 291.774380293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# help('sklearn.ensemble.RandomForestRegressor')\n",
    "#<WRITE CODE HERE FOR RANDOM FORESTS>\n",
    "\n",
    "%time _ = rf.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_rf = rf.predict(Xval)\n",
    "\n",
    "print \"MAE for Random Forest with n_estimators={} (default) is {}\".format(rf.n_estimators, mae(yval_pred_rf, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Random Forests. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of n_estimators obtained with Grid Search is: {'n_estimators': 13} with 284.178279742 MAE.\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR RANDOM FORESTS HYPER-PARAMETER TUNING>\n",
    "\n",
    "# The default value of n_estimators is 10, so we try 10 values around that number, i.e. from 5 to 15. \n",
    "\n",
    "param_grid_rf = {'n_estimators': range(5, 15)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(), \n",
    "                   param_grid_rf,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "rf_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of n_estimators obtained with Grid Search is: \" + str(rf_grid.best_params_) + \" with \" + str(-rf_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Gradient Tree Boosting (GB) with default parameters. A GB is also an ensemble technique based on Decision Trees. In this case, the second decision tree tries to fix the mistakes of the first decision tree. The third decision tree tries to fix the mistakes of the first two decision trees. An so on.\n",
    "\n",
    "Please, bear in mind that a GB with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.6 s\n",
      "MAE for Gradient Tree Boosting with n_estimators=100 (default) is 279.850958585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "# help('sklearn.ensemble.GradientBoostingRegressor')\n",
    "#<WRITE CODE HERE FOR GRADIENT BOOSTING>\n",
    "\n",
    "%time _ = gb.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_gb = gb.predict(Xval)\n",
    "\n",
    "print \"MAE for Gradient Tree Boosting with n_estimators={} (default) is {}\".format(gb.n_estimators, mae(yval_pred_gb, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Gradient Boosting. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of n_estimators obtained with Grid Search is: {'n_estimators': 140} with 278.345845545 MAE.\n"
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR GRADIENT BOOSTING HYPER-PARAMETER TUNING>\n",
    "\n",
    "#param_grid_gb = {'n_estimators': [50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]}\n",
    "param_grid_gb = {'n_estimators': range(50, 150, 10)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(), \n",
    "                   param_grid_gb,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "gb_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of n_estimators obtained with Grid Search is: \" + str(gb_grid.best_params_) + \" with \" + str(-gb_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should know which model performs best, and what hyper-parameters to use. Please, evaluate that best performing model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* MAE for KNN with K=5 (default) is 486.911414935\n",
    "* MAE for KNN with K=17 and weights=distance is 469.147029647\n",
    "\n",
    "\n",
    "* MAE for decision tree regressor with max_depth=None and min_samples_leaf=1 (default) is 486.911414935\n",
    "* MAE for decision tree regressor with max_depth=5 and min_samples_leaff=6 is 303.111990619\n",
    "\n",
    "\n",
    "* MAE for Random Forest with n_estimators=10 (default) is 291.774380293\n",
    "* MAE for Random Forest with n_estimators=13 is 284.178279742\n",
    "\n",
    "\n",
    "* MAE for Gradient Tree Boosting with n_estimators=100 (default) is 279.850958585\n",
    "* MAE for Gradient Tree Boosting with n_estimators=140 is 278.345845545  ---> BEST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 550 and  input n_features is 544 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-23e556dc3569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgb_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mytest_pred_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"MAE for Gradient Tree Boosting with n_estimators={} (default) is {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest_pred_gb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Clara\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \"\"\"\n\u001b[1;32m   1781\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Clara\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[1;31m# for use in inner loop, not raveling the output in single-class case,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[1;31m# not doing input validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m         \u001b[0mpredict_stages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Clara\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_init_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[1;34m\"\"\"Check input and compute prediction of ``init``. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[0;32mC:\\Users\\Clara\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    374\u001b[0m                              \u001b[1;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                              \u001b[1;34m\" input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 550 and  input n_features is 544 "
     ]
    }
   ],
   "source": [
    "#<WRITE CODE HERE FOR BEST MODEL EVALUATION ON THE TEST SET>\n",
    "\n",
    "gb_final = GradientBoostingRegressor(n_estimators=140)\n",
    "\n",
    "gb_final.fit(Xtrain, ytrain)\n",
    "\n",
    "ytest_pred_gb = gb_final.predict(Xtest)\n",
    "\n",
    "print \"MAE for Gradient Tree Boosting with n_estimators={} is {}\".format(gb_final.n_estimators, mae(ytest_pred_gb, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This section is more open-ended than the previous ones, and I offer less guidance. It is definitely harder, but you can always ask the teacher. \n",
    "\n",
    "You have to answer the following question: \n",
    "\n",
    "- \"Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many? Is it enough to have the attributes for the actual Sotavento location? (13th in the grid)\"\n",
    "\n",
    "In order to answer this question:\n",
    "\n",
    "1) Go through the \"Attribute Selection\" ipython notebook, and understand the main ideas about **SelectKBest** and **Pipeline**.\n",
    "\n",
    "2) Use **SelectKBest** and **Pipeline** (and whatever else you need) in order to find a subset of attributes that allows to build an accurate Decision Tree model. We are going to use here Decision Trees because they are faster (even if Random Forests or Gradient Boosting performed better in previous sections). Please, note that you cannot just copy/paste from the \"Attribute Selection\" notebook. You will have to think about how to use the main ideas from that notebook, and change whatever needs changing. \n",
    "\n",
    "3) Once you have decided which attributes should be used for the Decision Tree, evaluate the final model on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<USE AS MANY CELLS AS YOU NEED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 4950 candidates, totalling 4950 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    4.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   20.4s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   52.6s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.4min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  5.9min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  9.6min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 14.8min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed: 22.0min\n",
      "[Parallel(n_jobs=1)]: Done 4950 out of 4950 | elapsed: 31.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of max_depth obtained with Grid Search is: {'feature_selection__k': 536, 'regression__max_depth': 5} with 309.557283099 MAE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "param_grid = {'feature_selection__k': range(1, Xtrain.shape[1]+1),\n",
    "              'regression__max_depth': range(1, 10)}\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression)),\n",
    "  ('regression', tree.DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "clf_grid = GridSearchCV(clf, \n",
    "                        param_grid,\n",
    "                        scoring='mean_absolute_error',\n",
    "                        cv=partitions , n_jobs=1, verbose=1)\n",
    "\n",
    "clf_grid.fit(Xava, yava) \n",
    "\n",
    "print(\"The optimal value of max_depth obtained with Grid Search is: \" + str(clf_grid.best_params_) + \" with \" + str(-clf_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
