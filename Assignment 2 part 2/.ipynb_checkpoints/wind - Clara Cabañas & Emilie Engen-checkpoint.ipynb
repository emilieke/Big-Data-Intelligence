{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clara Caba√±as Pujadas and Emilie Krutnes Engen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND ASSIGNMENT PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SOME PRELIMINARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Emilie/Dropbox/Skole/UC3M/Machine Learning/Assignment 2 part 2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "# For Panda dataframes. A dataframe is a matrix-like structure, \n",
    "# similar to R dataframes  \n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['energy',\n",
       " 'steps',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'p54.162.1',\n",
       " 'p54.162.2',\n",
       " 'p54.162.3',\n",
       " 'p54.162.4',\n",
       " 'p54.162.5',\n",
       " 'p54.162.6',\n",
       " 'p54.162.7',\n",
       " 'p54.162.8',\n",
       " 'p54.162.9',\n",
       " 'p54.162.10',\n",
       " 'p54.162.11',\n",
       " 'p54.162.12',\n",
       " 'p54.162.13',\n",
       " 'p54.162.14',\n",
       " 'p54.162.15',\n",
       " 'p54.162.16',\n",
       " 'p54.162.17',\n",
       " 'p54.162.18',\n",
       " 'p54.162.19',\n",
       " 'p54.162.20',\n",
       " 'p54.162.21',\n",
       " 'p54.162.22',\n",
       " 'p54.162.23',\n",
       " 'p54.162.24',\n",
       " 'p54.162.25',\n",
       " 'p55.162.1',\n",
       " 'p55.162.2',\n",
       " 'p55.162.3',\n",
       " 'p55.162.4',\n",
       " 'p55.162.5',\n",
       " 'p55.162.6',\n",
       " 'p55.162.7',\n",
       " 'p55.162.8',\n",
       " 'p55.162.9',\n",
       " 'p55.162.10',\n",
       " 'p55.162.11',\n",
       " 'p55.162.12',\n",
       " 'p55.162.13',\n",
       " 'p55.162.14',\n",
       " 'p55.162.15',\n",
       " 'p55.162.16',\n",
       " 'p55.162.17',\n",
       " 'p55.162.18',\n",
       " 'p55.162.19',\n",
       " 'p55.162.20',\n",
       " 'p55.162.21',\n",
       " 'p55.162.22',\n",
       " 'p55.162.23',\n",
       " 'p55.162.24',\n",
       " 'p55.162.25',\n",
       " 'cape.1',\n",
       " 'cape.2',\n",
       " 'cape.3',\n",
       " 'cape.4',\n",
       " 'cape.5',\n",
       " 'cape.6',\n",
       " 'cape.7',\n",
       " 'cape.8',\n",
       " 'cape.9',\n",
       " 'cape.10',\n",
       " 'cape.11',\n",
       " 'cape.12',\n",
       " 'cape.13',\n",
       " 'cape.14',\n",
       " 'cape.15',\n",
       " 'cape.16',\n",
       " 'cape.17',\n",
       " 'cape.18',\n",
       " 'cape.19',\n",
       " 'cape.20',\n",
       " 'cape.21',\n",
       " 'cape.22',\n",
       " 'cape.23',\n",
       " 'cape.24',\n",
       " 'cape.25',\n",
       " 'p59.162.1',\n",
       " 'p59.162.2',\n",
       " 'p59.162.3',\n",
       " 'p59.162.4',\n",
       " 'p59.162.5',\n",
       " 'p59.162.6',\n",
       " 'p59.162.7',\n",
       " 'p59.162.8',\n",
       " 'p59.162.9',\n",
       " 'p59.162.10',\n",
       " 'p59.162.11',\n",
       " 'p59.162.12',\n",
       " 'p59.162.13',\n",
       " 'p59.162.14',\n",
       " 'p59.162.15',\n",
       " 'p59.162.16',\n",
       " 'p59.162.17',\n",
       " 'p59.162.18',\n",
       " 'p59.162.19',\n",
       " 'p59.162.20',\n",
       " 'p59.162.21',\n",
       " 'p59.162.22',\n",
       " 'p59.162.23',\n",
       " 'p59.162.24',\n",
       " 'p59.162.25',\n",
       " 'lai_lv.1',\n",
       " 'lai_lv.2',\n",
       " 'lai_lv.3',\n",
       " 'lai_lv.4',\n",
       " 'lai_lv.5',\n",
       " 'lai_lv.6',\n",
       " 'lai_lv.7',\n",
       " 'lai_lv.8',\n",
       " 'lai_lv.9',\n",
       " 'lai_lv.10',\n",
       " 'lai_lv.11',\n",
       " 'lai_lv.12',\n",
       " 'lai_lv.13',\n",
       " 'lai_lv.14',\n",
       " 'lai_lv.15',\n",
       " 'lai_lv.16',\n",
       " 'lai_lv.17',\n",
       " 'lai_lv.18',\n",
       " 'lai_lv.19',\n",
       " 'lai_lv.20',\n",
       " 'lai_lv.21',\n",
       " 'lai_lv.22',\n",
       " 'lai_lv.23',\n",
       " 'lai_lv.24',\n",
       " 'lai_lv.25',\n",
       " 'lai_hv.1',\n",
       " 'lai_hv.2',\n",
       " 'lai_hv.3',\n",
       " 'lai_hv.4',\n",
       " 'lai_hv.5',\n",
       " 'lai_hv.6',\n",
       " 'lai_hv.7',\n",
       " 'lai_hv.8',\n",
       " 'lai_hv.9',\n",
       " 'lai_hv.10',\n",
       " 'lai_hv.11',\n",
       " 'lai_hv.12',\n",
       " 'lai_hv.13',\n",
       " 'lai_hv.14',\n",
       " 'lai_hv.15',\n",
       " 'lai_hv.16',\n",
       " 'lai_hv.17',\n",
       " 'lai_hv.18',\n",
       " 'lai_hv.19',\n",
       " 'lai_hv.20',\n",
       " 'lai_hv.21',\n",
       " 'lai_hv.22',\n",
       " 'lai_hv.23',\n",
       " 'lai_hv.24',\n",
       " 'lai_hv.25',\n",
       " 'u10n.1',\n",
       " 'u10n.2',\n",
       " 'u10n.3',\n",
       " 'u10n.4',\n",
       " 'u10n.5',\n",
       " 'u10n.6',\n",
       " 'u10n.7',\n",
       " 'u10n.8',\n",
       " 'u10n.9',\n",
       " 'u10n.10',\n",
       " 'u10n.11',\n",
       " 'u10n.12',\n",
       " 'u10n.13',\n",
       " 'u10n.14',\n",
       " 'u10n.15',\n",
       " 'u10n.16',\n",
       " 'u10n.17',\n",
       " 'u10n.18',\n",
       " 'u10n.19',\n",
       " 'u10n.20',\n",
       " 'u10n.21',\n",
       " 'u10n.22',\n",
       " 'u10n.23',\n",
       " 'u10n.24',\n",
       " 'u10n.25',\n",
       " 'v10n.1',\n",
       " 'v10n.2',\n",
       " 'v10n.3',\n",
       " 'v10n.4',\n",
       " 'v10n.5',\n",
       " 'v10n.6',\n",
       " 'v10n.7',\n",
       " 'v10n.8',\n",
       " 'v10n.9',\n",
       " 'v10n.10',\n",
       " 'v10n.11',\n",
       " 'v10n.12',\n",
       " 'v10n.13',\n",
       " 'v10n.14',\n",
       " 'v10n.15',\n",
       " 'v10n.16',\n",
       " 'v10n.17',\n",
       " 'v10n.18',\n",
       " 'v10n.19',\n",
       " 'v10n.20',\n",
       " 'v10n.21',\n",
       " 'v10n.22',\n",
       " 'v10n.23',\n",
       " 'v10n.24',\n",
       " 'v10n.25',\n",
       " 'sp.1',\n",
       " 'sp.2',\n",
       " 'sp.3',\n",
       " 'sp.4',\n",
       " 'sp.5',\n",
       " 'sp.6',\n",
       " 'sp.7',\n",
       " 'sp.8',\n",
       " 'sp.9',\n",
       " 'sp.10',\n",
       " 'sp.11',\n",
       " 'sp.12',\n",
       " 'sp.13',\n",
       " 'sp.14',\n",
       " 'sp.15',\n",
       " 'sp.16',\n",
       " 'sp.17',\n",
       " 'sp.18',\n",
       " 'sp.19',\n",
       " 'sp.20',\n",
       " 'sp.21',\n",
       " 'sp.22',\n",
       " 'sp.23',\n",
       " 'sp.24',\n",
       " 'sp.25',\n",
       " 'stl1.1',\n",
       " 'stl1.2',\n",
       " 'stl1.3',\n",
       " 'stl1.4',\n",
       " 'stl1.5',\n",
       " 'stl1.6',\n",
       " 'stl1.7',\n",
       " 'stl1.8',\n",
       " 'stl1.9',\n",
       " 'stl1.10',\n",
       " 'stl1.11',\n",
       " 'stl1.12',\n",
       " 'stl1.13',\n",
       " 'stl1.14',\n",
       " 'stl1.15',\n",
       " 'stl1.16',\n",
       " 'stl1.17',\n",
       " 'stl1.18',\n",
       " 'stl1.19',\n",
       " 'stl1.20',\n",
       " 'stl1.21',\n",
       " 'stl1.22',\n",
       " 'stl1.23',\n",
       " 'stl1.24',\n",
       " 'stl1.25',\n",
       " 'u10.1',\n",
       " 'u10.2',\n",
       " 'u10.3',\n",
       " 'u10.4',\n",
       " 'u10.5',\n",
       " 'u10.6',\n",
       " 'u10.7',\n",
       " 'u10.8',\n",
       " 'u10.9',\n",
       " 'u10.10',\n",
       " 'u10.11',\n",
       " 'u10.12',\n",
       " 'u10.13',\n",
       " 'u10.14',\n",
       " 'u10.15',\n",
       " 'u10.16',\n",
       " 'u10.17',\n",
       " 'u10.18',\n",
       " 'u10.19',\n",
       " 'u10.20',\n",
       " 'u10.21',\n",
       " 'u10.22',\n",
       " 'u10.23',\n",
       " 'u10.24',\n",
       " 'u10.25',\n",
       " 'v10.1',\n",
       " 'v10.2',\n",
       " 'v10.3',\n",
       " 'v10.4',\n",
       " 'v10.5',\n",
       " 'v10.6',\n",
       " 'v10.7',\n",
       " 'v10.8',\n",
       " 'v10.9',\n",
       " 'v10.10',\n",
       " 'v10.11',\n",
       " 'v10.12',\n",
       " 'v10.13',\n",
       " 'v10.14',\n",
       " 'v10.15',\n",
       " 'v10.16',\n",
       " 'v10.17',\n",
       " 'v10.18',\n",
       " 'v10.19',\n",
       " 'v10.20',\n",
       " 'v10.21',\n",
       " 'v10.22',\n",
       " 'v10.23',\n",
       " 'v10.24',\n",
       " 'v10.25',\n",
       " 't2m.1',\n",
       " 't2m.2',\n",
       " 't2m.3',\n",
       " 't2m.4',\n",
       " 't2m.5',\n",
       " 't2m.6',\n",
       " 't2m.7',\n",
       " 't2m.8',\n",
       " 't2m.9',\n",
       " 't2m.10',\n",
       " 't2m.11',\n",
       " 't2m.12',\n",
       " 't2m.13',\n",
       " 't2m.14',\n",
       " 't2m.15',\n",
       " 't2m.16',\n",
       " 't2m.17',\n",
       " 't2m.18',\n",
       " 't2m.19',\n",
       " 't2m.20',\n",
       " 't2m.21',\n",
       " 't2m.22',\n",
       " 't2m.23',\n",
       " 't2m.24',\n",
       " 't2m.25',\n",
       " 'stl2.1',\n",
       " 'stl2.2',\n",
       " 'stl2.3',\n",
       " 'stl2.4',\n",
       " 'stl2.5',\n",
       " 'stl2.6',\n",
       " 'stl2.7',\n",
       " 'stl2.8',\n",
       " 'stl2.9',\n",
       " 'stl2.10',\n",
       " 'stl2.11',\n",
       " 'stl2.12',\n",
       " 'stl2.13',\n",
       " 'stl2.14',\n",
       " 'stl2.15',\n",
       " 'stl2.16',\n",
       " 'stl2.17',\n",
       " 'stl2.18',\n",
       " 'stl2.19',\n",
       " 'stl2.20',\n",
       " 'stl2.21',\n",
       " 'stl2.22',\n",
       " 'stl2.23',\n",
       " 'stl2.24',\n",
       " 'stl2.25',\n",
       " 'stl3.1',\n",
       " 'stl3.2',\n",
       " 'stl3.3',\n",
       " 'stl3.4',\n",
       " 'stl3.5',\n",
       " 'stl3.6',\n",
       " 'stl3.7',\n",
       " 'stl3.8',\n",
       " 'stl3.9',\n",
       " 'stl3.10',\n",
       " 'stl3.11',\n",
       " 'stl3.12',\n",
       " 'stl3.13',\n",
       " 'stl3.14',\n",
       " 'stl3.15',\n",
       " 'stl3.16',\n",
       " 'stl3.17',\n",
       " 'stl3.18',\n",
       " 'stl3.19',\n",
       " 'stl3.20',\n",
       " 'stl3.21',\n",
       " 'stl3.22',\n",
       " 'stl3.23',\n",
       " 'stl3.24',\n",
       " 'stl3.25',\n",
       " 'iews.1',\n",
       " 'iews.2',\n",
       " 'iews.3',\n",
       " 'iews.4',\n",
       " 'iews.5',\n",
       " 'iews.6',\n",
       " 'iews.7',\n",
       " 'iews.8',\n",
       " 'iews.9',\n",
       " 'iews.10',\n",
       " 'iews.11',\n",
       " 'iews.12',\n",
       " 'iews.13',\n",
       " 'iews.14',\n",
       " 'iews.15',\n",
       " 'iews.16',\n",
       " 'iews.17',\n",
       " 'iews.18',\n",
       " 'iews.19',\n",
       " 'iews.20',\n",
       " 'iews.21',\n",
       " 'iews.22',\n",
       " 'iews.23',\n",
       " 'iews.24',\n",
       " 'iews.25',\n",
       " 'inss.1',\n",
       " 'inss.2',\n",
       " 'inss.3',\n",
       " 'inss.4',\n",
       " 'inss.5',\n",
       " 'inss.6',\n",
       " 'inss.7',\n",
       " 'inss.8',\n",
       " 'inss.9',\n",
       " 'inss.10',\n",
       " 'inss.11',\n",
       " 'inss.12',\n",
       " 'inss.13',\n",
       " 'inss.14',\n",
       " 'inss.15',\n",
       " 'inss.16',\n",
       " 'inss.17',\n",
       " 'inss.18',\n",
       " 'inss.19',\n",
       " 'inss.20',\n",
       " 'inss.21',\n",
       " 'inss.22',\n",
       " 'inss.23',\n",
       " 'inss.24',\n",
       " 'inss.25',\n",
       " 'stl4.1',\n",
       " 'stl4.2',\n",
       " 'stl4.3',\n",
       " 'stl4.4',\n",
       " 'stl4.5',\n",
       " 'stl4.6',\n",
       " 'stl4.7',\n",
       " 'stl4.8',\n",
       " 'stl4.9',\n",
       " 'stl4.10',\n",
       " 'stl4.11',\n",
       " 'stl4.12',\n",
       " 'stl4.13',\n",
       " 'stl4.14',\n",
       " 'stl4.15',\n",
       " 'stl4.16',\n",
       " 'stl4.17',\n",
       " 'stl4.18',\n",
       " 'stl4.19',\n",
       " 'stl4.20',\n",
       " 'stl4.21',\n",
       " 'stl4.22',\n",
       " 'stl4.23',\n",
       " 'stl4.24',\n",
       " 'stl4.25',\n",
       " 'fsr.1',\n",
       " 'fsr.2',\n",
       " 'fsr.3',\n",
       " 'fsr.4',\n",
       " 'fsr.5',\n",
       " 'fsr.6',\n",
       " 'fsr.7',\n",
       " 'fsr.8',\n",
       " 'fsr.9',\n",
       " 'fsr.10',\n",
       " 'fsr.11',\n",
       " 'fsr.12',\n",
       " 'fsr.13',\n",
       " 'fsr.14',\n",
       " 'fsr.15',\n",
       " 'fsr.16',\n",
       " 'fsr.17',\n",
       " 'fsr.18',\n",
       " 'fsr.19',\n",
       " 'fsr.20',\n",
       " 'fsr.21',\n",
       " 'fsr.22',\n",
       " 'fsr.23',\n",
       " 'fsr.24',\n",
       " 'fsr.25',\n",
       " 'flsr.1',\n",
       " 'flsr.2',\n",
       " 'flsr.3',\n",
       " 'flsr.4',\n",
       " 'flsr.5',\n",
       " 'flsr.6',\n",
       " 'flsr.7',\n",
       " 'flsr.8',\n",
       " 'flsr.9',\n",
       " 'flsr.10',\n",
       " 'flsr.11',\n",
       " 'flsr.12',\n",
       " 'flsr.13',\n",
       " 'flsr.14',\n",
       " 'flsr.15',\n",
       " 'flsr.16',\n",
       " 'flsr.17',\n",
       " 'flsr.18',\n",
       " 'flsr.19',\n",
       " 'flsr.20',\n",
       " 'flsr.21',\n",
       " 'flsr.22',\n",
       " 'flsr.23',\n",
       " 'flsr.24',\n",
       " 'flsr.25',\n",
       " 'u100.1',\n",
       " 'u100.2',\n",
       " 'u100.3',\n",
       " 'u100.4',\n",
       " 'u100.5',\n",
       " 'u100.6',\n",
       " 'u100.7',\n",
       " 'u100.8',\n",
       " 'u100.9',\n",
       " 'u100.10',\n",
       " 'u100.11',\n",
       " 'u100.12',\n",
       " 'u100.13',\n",
       " 'u100.14',\n",
       " 'u100.15',\n",
       " 'u100.16',\n",
       " 'u100.17',\n",
       " 'u100.18',\n",
       " 'u100.19',\n",
       " 'u100.20',\n",
       " 'u100.21',\n",
       " 'u100.22',\n",
       " 'u100.23',\n",
       " 'u100.24',\n",
       " 'u100.25',\n",
       " 'v100.1',\n",
       " 'v100.2',\n",
       " 'v100.3',\n",
       " 'v100.4',\n",
       " 'v100.5',\n",
       " 'v100.6',\n",
       " 'v100.7',\n",
       " 'v100.8',\n",
       " 'v100.9',\n",
       " 'v100.10',\n",
       " 'v100.11',\n",
       " 'v100.12',\n",
       " 'v100.13',\n",
       " 'v100.14',\n",
       " 'v100.15',\n",
       " 'v100.16',\n",
       " 'v100.17',\n",
       " 'v100.18',\n",
       " 'v100.19',\n",
       " 'v100.20',\n",
       " 'v100.21',\n",
       " 'v100.22',\n",
       " 'v100.23',\n",
       " 'v100.24',\n",
       " 'v100.25']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including \n",
    "# the outcome to be predicted)\n",
    "print data.shape\n",
    "data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below, data is going to be separated in train, validation, and test. Given that the use of Pandas dataframes is quite advanced, and doing this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indicesTrain = (np.where(data.year<=2006))[0]\n",
    "indicesVal = (np.where((data.year==2007) | (data.year==2008)))[0]\n",
    "indicesTest = (np.where(data.year>=2009))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware!, **indicesTrain** does not contain the training data, but the *indices* of the training data. For instance, the following cell means that training data is made of instance number 0, instance number 1, ..., up to instance number 2527. This will be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2525, 2526, 2527])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicesTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to transform **data**, which is a Pandas dataframe, to **ava**, which is a NumPy matrix. The reason is that Scikit-learn uses NumPy matrices, not Panda dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ava = data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **ava** is going to be decomposed into inputs **X** and outputs **y**. And then, into training, validation, and test. For instance, **Xava** and **yava** contain the input attributes, and the output attribute (**energy**) of the whole dataset. Please, ask yourself why the inputs use \"6:\" and the output use \"0\". **Xtrain** and **ytrain** are the same, but for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xava = ava[:,6:]; yava = ava[:,0]\n",
    "Xtrain = ava[indicesTrain,6:]; ytrain = ava[indicesTrain,0]\n",
    "Xval = ava[indicesVal,6:]; yval = ava[indicesVal,0]\n",
    "Xtest = ava[indicesTest,6:]; ytest = ava[indicesTest,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following cell defines function **mae** (Mean Absolute Error), that we will use later to measure the accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mae(yval_pred, yval):\n",
    "  val_mae = metrics.mean_absolute_error(yval_pred, yval)\n",
    "  return(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains KNN with (Xtrain, ytrain) and evaluates it with (Xval, yval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.4 ms, sys: 19.3 ms, total: 108 ms\n",
      "Wall time: 155 ms\n",
      "MAE for KNN with K=5 is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "np.random.seed(0)\n",
    "\n",
    "%time _ = knn.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred = knn.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with K=5 is {}\".format(mae(yval_pred, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case you need help for KNN\n",
    "# help('sklearn.neighbors.KNeighborsRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell, does hyper-parameter tuning for parameter K (n_neighbors), from 1 to 4 by 1. Please, notice that with **partitions = [(indicesTrain, indicesVal)]** we are telling **gridSearch** to use the training dataset for training the different models with the different parameters, and the validation dataset for testing. Notice that this is different to other notebooks, where crossvalidation was used for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 134 ms, total: 2.22 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_neighbors': range(1,4,1)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "clf = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "%time _ = clf.fit(Xava,yava)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the best K parameter and the MAE of the final model built with the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: {'n_neighbors': 3} and MAE for best K: 503.711691044\n"
     ]
    }
   ],
   "source": [
    "print \"Best K: {} and MAE for best K: {}\".format(clf.best_params_, -clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HOW LONG DOES IT TAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to have some estimation of how long your machine learning algorithm is going to take. In the next two cells, try to estimate how many seconds KNN (with K=3) does it take, with only **100 instances**. With 6000 instances, it will take approximately 60 times that number. You can use **%time** for timing, as in previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.91 ms, sys: 1.25 ms, total: 4.17 ms\n",
      "Wall time: 7.94 ms\n"
     ]
    }
   ],
   "source": [
    "# Timing of KNN with 100 instances\n",
    "\n",
    "n_neighbors = 3\n",
    "Xtrain_100 = ava[:100,6:]; ytrain_100 = ava[:100,0]\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = knn.fit(Xtrain_100, ytrain_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do the same for Decision trees with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 ms, sys: 3.35 ms, total: 53.1 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "# Timing of Decision Trees with 100 instances\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "reg = tree.DecisionTreeRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = reg.fit(Xtrain_100, ytrain_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56 ms\n",
      "MAE for KNN with K=5 (default) is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with default parameters\n",
    "\n",
    "knn_def = neighbors.KNeighborsRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = knn_def.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_knn_def = knn_def.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with K={} (default) is {}\".format(knn_def.n_neighbors, mae(yval_pred_knn_def, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for KNN. Can you improve results? Note: if **gridSearch** takes too long, you can use **Randomized Search** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 38 candidates, totalling 38 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   22.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal combination of parameters obtained with Grid Search is: {'n_neighbors': 17, 'weights': 'distance'} with 469.147029647 MAE.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning for KNN with Grid Search\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': range(1,20,1),\n",
    "    'weights' : ['uniform', 'distance']   \n",
    "}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "knn_grid = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid_knn,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "knn_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal combination of parameters obtained with Grid Search is: \" + str(knn_grid.best_params_) + \" with \" + str(-knn_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "We do improve the results, because the Mean Absolute Error is reduced.\n",
    "* We obtained a MAE = 486.911414935 with K=5 (default).\n",
    "* We obtain a MAE = 469.147029647 with K=17 (optimal parameter obtained with Grid Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree for regression with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.06 s\n",
      "MAE for decision tree regressor with max_depth=None and min_samples_leaf=1 (default) is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "# Train a decision tree with default parameters\n",
    "\n",
    "reg_def = tree.DecisionTreeRegressor()\n",
    "\n",
    "np.random.seed(0)\n",
    "%time _ = reg_def.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_reg_def = reg_def.predict(Xval)\n",
    "\n",
    "print \"MAE for decision tree regressor with max_depth={} and min_samples_leaf={} (default) is {}\".format(reg_def.max_depth, reg_def.min_samples_leaf, mae(yval_pred_knn_def, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Decision trees. Can you improve results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 81 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   27.2s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of max_depth and min_samples_leaf obtained with Grid Search is: {'max_depth': 5, 'min_samples_leaf': 6} with 303.459603203 MAE.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning for decision trees\n",
    "\n",
    "param_grid_reg = {\n",
    "    'max_depth': range(1,10,1),\n",
    "    'min_samples_leaf': range(1,10,1) \n",
    "}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "reg_grid = GridSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid_reg,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "reg_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of max_depth and min_samples_leaf obtained with Grid Search is: \" + str(reg_grid.best_params_) + \" with \" + str(-reg_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "We do improve the results, because the Mean Absolute Error is reduced.\n",
    "* We obtained a MAE = 486.911414935 with max_depth=None and min_samples_leaf=1 (default).\n",
    "* We obtain a MAE = 303.459603203 max_depth=5 and min_samples_leaf=6 (optimal parameter obtained with Grid Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest (RF) with default parameters. A RF is an ensemble technique based on Decision Trees, but instead of training just a single decision tree, it trains many of them and then computes the average of the outputs. Please, bear in mind that a RF with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 119 ms, total: 16.3 s\n",
      "Wall time: 18 s\n",
      "MAE for Random Forest with n_estimators=10 (default) is 287.671359507\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest with default parameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "%time _ = rf.fit(Xtrain, ytrain)\n",
    "yval_pred_rf = rf.predict(Xval)\n",
    "\n",
    "print \"MAE for Random Forest with n_estimators={} (default) is {}\".format(rf.n_estimators, mae(yval_pred_rf, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Random Forests. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of n_estimators obtained with Grid Search is: {'n_estimators': 13} with 283.199414342 MAE.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning for Random Forest\n",
    "\n",
    "# The default value of n_estimators is 10, so we try 10 values around that number, i.e. from 5 to 15. \n",
    "\n",
    "param_grid_rf = {'n_estimators': range(5, 15)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(), \n",
    "                   param_grid_rf,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "rf_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of n_estimators obtained with Grid Search is: \" + str(rf_grid.best_params_) + \" with \" + str(-rf_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Gradient Tree Boosting (GB) with default parameters. A GB is also an ensemble technique based on Decision Trees. In this case, the second decision tree tries to fix the mistakes of the first decision tree. The third decision tree tries to fix the mistakes of the first two decision trees. An so on.\n",
    "\n",
    "Please, bear in mind that a GB with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 304 ms, total: 25.8 s\n",
      "Wall time: 30.5 s\n",
      "MAE for Gradient Tree Boosting with n_estimators=100 (default) is 280.170371317\n"
     ]
    }
   ],
   "source": [
    "# Train a Gradient Tree Boosting with default parameters\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "%time _ = gb.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_pred_gb = gb.predict(Xval)\n",
    "\n",
    "print \"MAE for Gradient Tree Boosting with n_estimators={} (default) is {}\".format(gb.n_estimators, mae(yval_pred_gb, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Gradient Boosting. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of n_estimators obtained with Grid Search is: {'n_estimators': 120} with 279.28141778 MAE.\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning for Gradient Boosting\n",
    "\n",
    "param_grid_gb = {'n_estimators': range(50, 150, 10)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(), \n",
    "                   param_grid_gb,\n",
    "                   scoring='mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "\n",
    "gb_grid.fit(Xava, yava)\n",
    "\n",
    "print(\"The optimal value of n_estimators obtained with Grid Search is: \" + str(gb_grid.best_params_) + \" with \" + str(-gb_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should know which model performs best, and what hyper-parameters to use. Please, evaluate that best performing model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* MAE for KNN with K=5 (default) is 486.911414935\n",
    "* MAE for KNN with K=17 and weights=distance is 469.147029647\n",
    "\n",
    "\n",
    "* MAE for decision tree regressor with max_depth=None and min_samples_leaf=1 (default) is 486.911414935\n",
    "* MAE for decision tree regressor with max_depth=5 and min_samples_leaf=6 is 303.111990619\n",
    "\n",
    "\n",
    "* MAE for Random Forest with n_estimators=10 (default) is 287.671359507\n",
    "* MAE for Random Forest with n_estimators=13 is 283.199414342\n",
    "\n",
    "\n",
    "* MAE for Gradient Tree Boosting with n_estimators=100 (default) is 280.170371317\n",
    "* MAE for Gradient Tree Boosting with n_estimators=140 is 279.28141778  ---> BEST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Gradient Tree Boosting with n_estimators=120 is 294.623890308\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Gradient Tree Boosting\n",
    "\n",
    "gb_final = GradientBoostingRegressor(n_estimators=120)\n",
    "\n",
    "gb_final.fit(Xtrain, ytrain)\n",
    "\n",
    "ytest_pred_gb = gb_final.predict(Xtest)\n",
    "\n",
    "print \"MAE for Gradient Tree Boosting with n_estimators={} is {}\".format(gb_final.n_estimators, mae(ytest_pred_gb, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This section is more open-ended than the previous ones, and I offer less guidance. It is definitely harder, but you can always ask the teacher. \n",
    "\n",
    "You have to answer the following question: \n",
    "\n",
    "- \"Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many? Is it enough to have the attributes for the actual Sotavento location? (13th in the grid)\"\n",
    "\n",
    "In order to answer this question:\n",
    "\n",
    "1) Go through the \"Attribute Selection\" ipython notebook, and understand the main ideas about **SelectKBest** and **Pipeline**.\n",
    "\n",
    "2) Use **SelectKBest** and **Pipeline** (and whatever else you need) in order to find a subset of attributes that allows to build an accurate Decision Tree model. We are going to use here Decision Trees because they are faster (even if Random Forests or Gradient Boosting performed better in previous sections). Please, note that you cannot just copy/paste from the \"Attribute Selection\" notebook. You will have to think about how to use the main ideas from that notebook, and change whatever needs changing. \n",
    "\n",
    "3) Once you have decided which attributes should be used for the Decision Tree, evaluate the final model on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 11000 candidates, totalling 11000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   23.1s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   55.8s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.2min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  5.2min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  7.9min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 11.7min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed: 16.7min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed: 23.3min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed: 32.3min\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed: 43.2min\n",
      "[Parallel(n_jobs=1)]: Done 8449 tasks       | elapsed: 56.7min\n",
      "[Parallel(n_jobs=1)]: Done 9799 tasks       | elapsed: 70.0min\n",
      "[Parallel(n_jobs=1)]: Done 11000 out of 11000 | elapsed: 82.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of max_depth obtained with Grid Search is: {'feature_selection__k': 278, 'regression__max_depth': 6, 'regression__min_samples_split': 22} with 307.893322029 MAE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "param_grid = {'feature_selection__k': np.arange(Xtrain.shape[1])+1,\n",
    "              'regression__max_depth': range(2,10,2),\n",
    "              'regression__min_samples_split': range(10,30,4)\n",
    "}\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression)),\n",
    "  ('regression', tree.DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "clf_grid = GridSearchCV(clf, \n",
    "                        param_grid,\n",
    "                        scoring='mean_absolute_error',\n",
    "                        cv=partitions , n_jobs=1, verbose=1)\n",
    "\n",
    "clf_grid.fit(Xava, yava) \n",
    "\n",
    "print(\"The optimal value of max_depth obtained with Grid Search is: \" + str(clf_grid.best_params_) + \" with \" + str(-clf_grid.best_score_) + \" MAE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain_sotavento = Xtrain[:, range(18,556,25)]\n",
    "Xval_sotavento  = Xval[:, range(18,556,25)]\n",
    "Xtest_sotavento = Xtest[:, range(18,556,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395.984980754\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor()\n",
    "clf.fit(Xtrain_sotavento, ytrain)\n",
    "\n",
    "y_sotavento_pred = clf.predict(Xval_sotavento)\n",
    "print mae(y_sotavento_pred, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "The MAE obtained using only the Sotavento variables is very close to the one obtained using the K best selected features, and also very close to the optimal MAE obtained before with all the variables.\n",
    "\n",
    "Thus, the 25 Sotavento variables are enough to make a good prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389.15457346\n"
     ]
    }
   ],
   "source": [
    "# Now we obtain the MAE for the Test partition\n",
    "\n",
    "y_sotavento_pred_test = clf.predict(Xtest_sotavento)\n",
    "print mae(y_sotavento_pred_test, ytest)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
