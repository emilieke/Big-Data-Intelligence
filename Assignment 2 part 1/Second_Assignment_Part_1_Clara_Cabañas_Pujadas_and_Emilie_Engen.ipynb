{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clara Caba√±as Pujadas and Emilie Krutnes Engen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND ASSIGNMENT. MACHINE LEARNING WITH SCIKIT-LEARN. \n",
    "## PART I (1 POINT)\n",
    "\n",
    "The aim of part I of the Scikit-learn assignment is for you to self-learn and get used to this Machine Learning tool. The main part (part II) of the assignment will be explained next week (11/12). \n",
    "\n",
    "Here, you will learn to:\n",
    "\n",
    "- Perform a crossvalidation on the iris classification problem with decision trees (so far, we have only done regression)\n",
    "- Perform a crossvalidation on the iris classification problem **with KNN** (I haven't explained this, you will have to learn how to use it from the web)\n",
    "- Perform grid search in order to determine the best value for hyper-parameter K\n",
    "\n",
    "You will also have to go through two notebooks I have prepared for you in order to see how crossvalidation and hyper-parameter tuning are used in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Carry out the \"DECISION TREES WITH A TRAINING AND A TESTING SET AND CROSSVALIDATION\" notebook and understand the main ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform a crossvalidation on the iris classification problem with decision trees:\n",
    "\n",
    "** It is important to remember that for classification, you have to use**\n",
    "- clf = tree.DecisionTreeClassifier() # for constructing the classifier\n",
    "- metrics.accuracy # for computing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          0.93333333  0.8         1.          1.\n",
      "  0.93333333  1.          1.          0.93333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris data \n",
    "iris = load_iris()\n",
    "\n",
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create a k-fold cross validation iterator of k=10 folds\n",
    "cv_tree = KFold(iris.data.shape[0], 10, shuffle=True)\n",
    "\n",
    "# Computing accuracy scores\n",
    "scores_tree = cross_val_score(tree.DecisionTreeClassifier(), \n",
    "                         iris.data, iris.target, \n",
    "                         scoring='accuracy', \n",
    "                         cv = cv_tree) \n",
    "\n",
    "# Printing the 10 scores\n",
    "print(scores_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform a crossvalidation on the iris classification problem with KNN\n",
    "\n",
    "I haven't explained how to use KNN in Scikit-learn. You will have to read and obtain the relevant information [here](http://scikit-learn.org/stable/modules/neighbors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          0.93333333  0.8         1.          1.          1.\n",
      "  1.          0.93333333  0.93333333]\n",
      "\n",
      "The mean accuracy for the deafult number of neighbours is: 0.960000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create a k-fold cross validation iterator of k=10 folds\n",
    "cv_knn = KFold(iris.data.shape[0], 10, shuffle=True)\n",
    "\n",
    "# Computing accuracy scores\n",
    "scores_knn = cross_val_score(KNeighborsClassifier(), \n",
    "                         iris.data, iris.target, \n",
    "                         scoring='accuracy', \n",
    "                         cv = cv_knn) \n",
    "\n",
    "# Printing the 10 scores\n",
    "print(scores_knn)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print(\"\\nThe mean accuracy for the deafult number of neighbours is: %f\" % np.mean(scores_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try different values for K (KNN) - change them by hand- and see if you obtain a better result than with KNN default value. Always use crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy for 1 neighbours is: 0.960000\n",
      "The mean accuracy for 2 neighbours is: 0.946667\n",
      "The mean accuracy for 3 neighbours is: 0.960000\n",
      "The mean accuracy for 4 neighbours is: 0.946667\n",
      "The mean accuracy for 5 neighbours is: 0.960000\n",
      "The mean accuracy for 6 neighbours is: 0.960000\n",
      "The mean accuracy for 7 neighbours is: 0.953333\n",
      "The mean accuracy for 8 neighbours is: 0.953333\n",
      "The mean accuracy for 9 neighbours is: 0.953333\n",
      "The mean accuracy for 10 neighbours is: 0.953333\n",
      "The mean accuracy for 11 neighbours is: 0.960000\n",
      "The mean accuracy for 12 neighbours is: 0.953333\n",
      "The mean accuracy for 13 neighbours is: 0.966667\n",
      "The mean accuracy for 14 neighbours is: 0.966667\n",
      "The mean accuracy for 15 neighbours is: 0.966667\n",
      "The mean accuracy for 16 neighbours is: 0.953333\n",
      "The mean accuracy for 17 neighbours is: 0.960000\n",
      "The mean accuracy for 18 neighbours is: 0.953333\n",
      "The mean accuracy for 19 neighbours is: 0.966667\n"
     ]
    }
   ],
   "source": [
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create a k-fold cross validation iterator of k=10 folds\n",
    "cv_knn = KFold(iris.data.shape[0], 10, shuffle=True)\n",
    "\n",
    "# Computing accuracy score for number of neighbors ranging from 1 to 19\n",
    "for i in range(1, 20):\n",
    "    scores_knn = cross_val_score(KNeighborsClassifier(n_neighbors=i), \n",
    "                         iris.data, iris.target, \n",
    "                         scoring='accuracy', \n",
    "                         cv = cv_knn)\n",
    "    \n",
    "    # Print mean accuracy\n",
    "    print(\"The mean accuracy for %d neighbours is: %f\" % (i, np.mean(scores_knn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carry out THE \"DECISION TREE HYPER-PARAMETERS. TUNING DECISION TREES\" notebook and understand the main ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Grid Search and Randomized Search to find the optimal value For K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of k obtained with Grid Search is: {'n_neighbors': 13} with 0.98 accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# LMake results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Using number of neighbors as hyper-parameter\n",
    "param_k_grid = {'n_neighbors': list(range(1,20))}\n",
    "\n",
    "# Grid search\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                   param_k_grid,\n",
    "                   scoring='accuracy',\n",
    "                   cv=10, n_jobs=1)\n",
    "\n",
    "# Fit the data\n",
    "knn_grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the optimal number of neighbors and accuracy score\n",
    "print(\"The optimal value of k obtained with Grid Search is: \" + str(knn_grid.best_params_) + \" with \" + str(knn_grid.best_score_) + \" accuracy.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of k obtained with Randomized Search is: {'n_neighbors': 13} with 0.98 accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Using number of neighbors as hyper-parameter\n",
    "param_k_rand = {'n_neighbors': list(range(1,20))}\n",
    "\n",
    "# Number of iterations\n",
    "n_iter_search = 10\n",
    "\n",
    "# Randomized search\n",
    "knn_rand = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                                   param_distributions=param_k_rand,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=10 , n_jobs=1,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "# Fit the data\n",
    "knn_rand.fit(iris.data,iris.target)\n",
    "\n",
    "# Print the optimal number of neighbors and accuracy score\n",
    "print(\"The optimal value of k obtained with Randomized Search is: \" + str(knn_rand.best_params_) + \" with \" + str(knn_rand.best_score_) + \" accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. K is the main hyper-parameter of KNN. Find another hyper-parameter that you consider relevant, and try to optimize both K and the other parameter using grid-search. Are you able to improve on previous results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using number of neighbors and algorithm as hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of k obtained with Grid Search is: {'n_neighbors': 13, 'algorithm': 'auto'} with 0.98 accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Using number of neighbors and weighting scheme as hyper-parameters\n",
    "param_k_grid = {'n_neighbors': list(range(1,20)),\n",
    "                'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "# Grid search\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                   param_k_grid,\n",
    "                   scoring='accuracy',\n",
    "                   cv=10, n_jobs=1)\n",
    "\n",
    "# Fit the data\n",
    "knn_grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the optimal number of neighbors and accuracy score\n",
    "print(\"The optimal value of k obtained with Grid Search is: \" + str(knn_grid.best_params_) + \" with \" + str(knn_grid.best_score_) + \" accuracy.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using number of neighbors, algorithm and power parameter as hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of k obtained with Grid Search is: {'n_neighbors': 8, 'algorithm': 'brute', 'p': 5} with 0.986666666667 accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Make results reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# Using number of neighbors and weighting scheme as hyper-parameters\n",
    "param_k_grid = {'n_neighbors': list(range(1,20)),\n",
    "                'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'p': list(range(1,10))}\n",
    "\n",
    "# Grid search\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                   param_k_grid,\n",
    "                   scoring='accuracy',\n",
    "                   cv=10, n_jobs=1)\n",
    "\n",
    "# Fit the data\n",
    "knn_grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the optimal number of neighbors and accuracy score\n",
    "print(\"The optimal value of k obtained with Grid Search is: \" + str(knn_grid.best_params_) + \" with \" + str(knn_grid.best_score_) + \" accuracy.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using number of neighbors, algorithm and power parameter as hyperparameters we were able to improve the accuracy by 0.0066667. We tried implemting the grid search by including several hyperparameters. However, this affected the computation time of the algorithm. Therefore when implementing this algorithm, both the accuracy score and the computation time should be taken into account. The inclusion of several hyperparameters are based on how you value the accuracy of the solution versus the running time of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
