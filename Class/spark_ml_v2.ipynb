{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "\n",
    "This notebook contains two sections:\n",
    "\n",
    "1. **Using decision trees in Spark MLlib:** MLlib is the old Spark Machine Learning library (Spark 1.x versions). It is based on the RDD data structure and, according to Databricks, it will be discontinued by Spark 3.0 version. This part of the notebook is for reference only, because RDD's are still used at the moment.\n",
    "2. **Using decision trees in SPARK ML:** MLlib is the new Spark Machine Learning library and it will be the standard from now on (Spark 2.x version). It is based on the new Dataframe data structure. Dataframes are very similar to R Dataframes or Python Pandas Dataframes. You can jump to this second section if you are in a hurry.\n",
    "3. **Using a pipeline in SPARK ML:** A pipeline is a method which is made of two or more methods. Each method can be a preprocessing step (i.e. feature selection, pca, ...) or a training algorithm (decision trees, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using decision trees in SPARK MLlib\n",
    "\n",
    "For more information on the MLlib RDD-based Spark library: [http://spark.apache.org/docs/latest/mllib-guide.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find how to use other algorithms (Random Forest, Gradient Boosting, etc.) [here](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "SPARK_HOME = \"\"\"C:/spark-2.1.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.4-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[*]\", appName=\"PythonDecisionTreeClassificationExample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aler.PC-164-148\\\\Google Drive\\\\TRABAJO LOCAL ALLWAYSSYNC\\\\DOCENCIA\\\\MASTER BIG DATA\\\\ACLASES 16-17\\\\misPruebasTerceraPractica'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's load the Boston dataset and check its description. Its data about housing prices depending on the characteristics of the zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "#print(boston.DESCR)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X will contain the input attributes, and y the output attribute. We can visualize the shape of the dataset (number of instances x number of input attributes), and the shape of the target (output) attribute.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L, 13L) (506L,)\n"
     ]
    }
   ],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "data = zip(y,X)\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we can see the first two instances (input and output attributes) of the dataset. Notice that data is a list of tuples (inputs, output).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=\n",
      "array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
      "          0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
      "          6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
      "          2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
      "          4.98000000e+00],\n",
      "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00,\n",
      "          0.00000000e+00,   4.69000000e-01,   6.42100000e+00,\n",
      "          7.89000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
      "          2.42000000e+02,   1.78000000e+01,   3.96900000e+02,\n",
      "          9.14000000e+00]])\n",
      "y=[ 24.   21.6]\n",
      "data=\n",
      "[(24.0,\n",
      "  array([  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
      "         0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
      "         6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
      "         2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
      "         4.98000000e+00])),\n",
      " (21.600000000000001,\n",
      "  array([  2.73100000e-02,   0.00000000e+00,   7.07000000e+00,\n",
      "         0.00000000e+00,   4.69000000e-01,   6.42100000e+00,\n",
      "         7.89000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
      "         2.42000000e+02,   1.78000000e+01,   3.96900000e+02,\n",
      "         9.14000000e+00]))]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print \"X=\"\n",
    "pprint.pprint(X[0:2])\n",
    "print \"y={}\".format(y[0:2])\n",
    "print \"data=\"\n",
    "pprint.pprint(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Let's convert the dataset into an Spark RDD. IMPORTANT: X and y were located into a SINGLE computer (driver). From now on, data_rdd is distributed among 4 partitions, which might be located in different computers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "data_rdd = sc.parallelize(data,4)\n",
    "print data_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we transform a RDD made of tuples into an RDD made of LabeledPoint. A LabeledPoint represents one instance, made of two elements. The first one contains the class or value to be predicted. The second one contains the features / attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_rdd = data_rdd.map(lambda x: LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(24.0, [0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]),\n",
      " LabeledPoint(21.6, [0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14]),\n",
      " LabeledPoint(34.7, [0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03])]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint (data_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the RDDs into 70% training and 30% test folds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData_rdd, testData_rdd) = data_rdd.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the regression tree on the training RDD**. \n",
    "More info in [https://spark.apache.org/docs/latest/mllib-decision-tree.html#classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categoricalFeaturesInfo={} because all features are continous\n",
    "model = DecisionTree.trainRegressor(trainingData_rdd, categoricalFeaturesInfo={}, maxDepth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the MAE on the test RDD. IMPORTANT: zip is used to make tuples (label, prediction). This assumes that both RDDs (testData_rdd and predictions_rdd are in the same order. According to the documentation, this is the case. But in general, we cannot count on two RDDs being distributed in the same way into different machines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 3.04358899663\n"
     ]
    }
   ],
   "source": [
    "predictions_rdd = model.predict(testData_rdd.map(lambda lp: lp.features))\n",
    "labelsAndPredictions = testData_rdd.map(lambda lp: lp.label).zip(predictions_rdd)\n",
    "testErr = labelsAndPredictions.map(lambda (v, p): abs(v-p)).mean()\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "model = RandomForest.trainRegressor(trainingData_rdd, categoricalFeaturesInfo={}, numTrees=10, featureSubsetStrategy=\"auto\", maxDepth=4, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 2.43709290525\n"
     ]
    }
   ],
   "source": [
    "predictions_rdd = model.predict(testData_rdd.map(lambda lp: lp.features))\n",
    "labelsAndPredictions = testData_rdd.map(lambda lp: lp.label).zip(predictions_rdd)\n",
    "testErr = labelsAndPredictions.map(lambda (v, p): abs(v-p)).mean()\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "model = GradientBoostedTrees.trainRegressor(trainingData_rdd, categoricalFeaturesInfo={}, numIterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 2.55481886843\n"
     ]
    }
   ],
   "source": [
    "predictions_rdd = model.predict(testData_rdd.map(lambda lp: lp.features))\n",
    "labelsAndPredictions = testData_rdd.map(lambda lp: lp.label).zip(predictions_rdd)\n",
    "testErr = labelsAndPredictions.map(lambda (v, p): abs(v-p)).mean()\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Using decision trees in ML SPARK\n",
    "\n",
    "ML is the Dataframe-based Spark library. This is the most modern library. For more information: [http://spark.apache.org/docs/latest/mllib-guide.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The next cell starts Spark**. A difference between RDD-based Spark MLlib (old version 1.x) and the Dataframe-based Spark ML (Spark version 2.x) is the entry point: now, _SparkSession_ is used to start spark. A SparkContext for old RDD-based MLlib can still be accessed via spark.sparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"C:/spark-2.1.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.4-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's load the Boston dataset and check its description. Its data about housing prices depending on the characteristics of the zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X will contain the input attributes, and y the output attribute. We can visualize the shape of the dataset (number of instances x number of input attributes), and the shape of the target (output) attribute.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L, 13L) (506L,)\n"
     ]
    }
   ],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names of attributes / features. _label_ is the output attribute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names=map(str,boston.feature_names)\n",
    "names = ['label']+names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, a Pandas Dataframe is created. This is not particularly important. This is done just as a preparation step to save the Boston dataset into a csv file because, commmonly, we always start reading data from a file.**\n",
    "\n",
    "Note: an issue which is important to understand is that a Pandas Dataframe is not an Spark Dataframe. The former resides into a single machine. The latter is split into several partitions / machines for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_pd = pd.DataFrame(X)\n",
    "data_pd.insert(0, 'label', y)\n",
    "data_pd.columns = names\n",
    "data_pd.to_csv(\"boston.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**The next cell reads the data (in csv format) into Spark.** _datasd_ is truly a Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sd = spark.read.csv(\"boston.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** _show_ is a Spark command to visualize the first rows of the Spark dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+\n",
      "|label|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|\n",
      "+-----+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+\n",
      "| 24.0|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|\n",
      "| 21.6|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|\n",
      "| 34.7|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|\n",
      "| 33.4|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|\n",
      "| 36.2|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|\n",
      "| 28.7|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|\n",
      "| 22.9|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|\n",
      "| 27.1|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|\n",
      "| 16.5|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|\n",
      "| 18.9|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|\n",
      "| 15.0|0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|\n",
      "| 18.9|0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|\n",
      "| 21.7|0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|\n",
      "| 20.4|0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|\n",
      "| 18.2|0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|\n",
      "| 19.9|0.62739| 0.0| 8.14| 0.0|0.538|5.834| 56.5|4.4986|4.0|307.0|   21.0|395.62| 8.47|\n",
      "| 23.1|1.05393| 0.0| 8.14| 0.0|0.538|5.935| 29.3|4.4986|4.0|307.0|   21.0|386.85| 6.58|\n",
      "| 17.5| 0.7842| 0.0| 8.14| 0.0|0.538| 5.99| 81.7|4.2579|4.0|307.0|   21.0|386.75|14.67|\n",
      "| 20.2|0.80271| 0.0| 8.14| 0.0|0.538|5.456| 36.6|3.7965|4.0|307.0|   21.0|288.99|11.69|\n",
      "| 18.2| 0.7258| 0.0| 8.14| 0.0|0.538|5.727| 69.5|3.7965|4.0|307.0|   21.0|390.95|11.28|\n",
      "+-----+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cell displays examples of Spark transformations:**\n",
    "\n",
    "- _select()_ to select some columns and create other columns\n",
    "- _filter()_ to filter some rows according to a condition\n",
    "\n",
    "and of Spark actions:\n",
    "\n",
    "- _show()_\n",
    "- _count()_\n",
    "\n",
    "**_repartition(4)_ is a command to distribute the dataframe into 4 partitions, which in principle, could reside into 4 different computers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- CRIM: double (nullable = true)\n",
      " |-- ZN: double (nullable = true)\n",
      " |-- INDUS: double (nullable = true)\n",
      " |-- CHAS: double (nullable = true)\n",
      " |-- NOX: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DIS: double (nullable = true)\n",
      " |-- RAD: double (nullable = true)\n",
      " |-- TAX: double (nullable = true)\n",
      " |-- PTRATIO: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- LSTAT: double (nullable = true)\n",
      "\n",
      "None\n",
      "[('label', 'double'), ('CRIM', 'double'), ('ZN', 'double'), ('INDUS', 'double'), ('CHAS', 'double'), ('NOX', 'double'), ('RM', 'double'), ('AGE', 'double'), ('DIS', 'double'), ('RAD', 'double'), ('TAX', 'double'), ('PTRATIO', 'double'), ('B', 'double'), ('LSTAT', 'double')]\n",
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "| 24.0|\n",
      "| 21.6|\n",
      "| 34.7|\n",
      "| 33.4|\n",
      "| 36.2|\n",
      "| 28.7|\n",
      "| 22.9|\n",
      "| 27.1|\n",
      "| 16.5|\n",
      "| 18.9|\n",
      "| 15.0|\n",
      "| 18.9|\n",
      "| 21.7|\n",
      "| 20.4|\n",
      "| 18.2|\n",
      "| 19.9|\n",
      "| 23.1|\n",
      "| 17.5|\n",
      "| 20.2|\n",
      "| 18.2|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-----+-----------+\n",
      "|label|(label + 1)|\n",
      "+-----+-----------+\n",
      "| 24.0|       25.0|\n",
      "| 21.6|       22.6|\n",
      "| 34.7|       35.7|\n",
      "| 33.4|       34.4|\n",
      "| 36.2|       37.2|\n",
      "| 28.7|       29.7|\n",
      "| 22.9|       23.9|\n",
      "| 27.1|       28.1|\n",
      "| 16.5|       17.5|\n",
      "| 18.9|       19.9|\n",
      "| 15.0|       16.0|\n",
      "| 18.9|       19.9|\n",
      "| 21.7|       22.7|\n",
      "| 20.4|       21.4|\n",
      "| 18.2|       19.2|\n",
      "| 19.9|       20.9|\n",
      "| 23.1|       24.1|\n",
      "| 17.5|       18.5|\n",
      "| 20.2|       21.2|\n",
      "| 18.2|       19.2|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-----+-------+----+-----+----+------+-----+-----+------+----+-----+-------+------+-----+\n",
      "|label|   CRIM|  ZN|INDUS|CHAS|   NOX|   RM|  AGE|   DIS| RAD|  TAX|PTRATIO|     B|LSTAT|\n",
      "+-----+-------+----+-----+----+------+-----+-----+------+----+-----+-------+------+-----+\n",
      "| 50.0|1.46336| 0.0|19.58| 0.0| 0.605|7.489| 90.8|1.9709| 5.0|403.0|   14.7|374.43| 1.73|\n",
      "| 50.0|1.83377| 0.0|19.58| 1.0| 0.605|7.802| 98.2|2.0407| 5.0|403.0|   14.7|389.61| 1.92|\n",
      "| 50.0|1.51902| 0.0|19.58| 1.0| 0.605|8.375| 93.9| 2.162| 5.0|403.0|   14.7|388.45| 3.32|\n",
      "| 50.0|2.01019| 0.0|19.58| 0.0| 0.605|7.929| 96.2|2.0459| 5.0|403.0|   14.7| 369.3|  3.7|\n",
      "| 50.0|0.05602| 0.0| 2.46| 0.0| 0.488|7.831| 53.6|3.1992| 3.0|193.0|   17.8|392.63| 4.45|\n",
      "| 50.0|0.01381|80.0| 0.46| 0.0| 0.422|7.875| 32.0|5.6484| 4.0|255.0|   14.4|394.23| 2.97|\n",
      "| 50.0|0.02009|95.0| 2.68| 0.0|0.4161|8.034| 31.9| 5.118| 4.0|224.0|   14.7|390.55| 2.88|\n",
      "| 50.0|0.52693| 0.0|  6.2| 0.0| 0.504|8.725| 83.0|2.8944| 8.0|307.0|   17.4| 382.0| 4.63|\n",
      "| 50.0|0.61154|20.0| 3.97| 0.0| 0.647|8.704| 86.9| 1.801| 5.0|264.0|   13.0| 389.7| 5.12|\n",
      "| 50.0|0.57834|20.0| 3.97| 0.0| 0.575|8.297| 67.0|2.4216| 5.0|264.0|   13.0|384.54| 7.44|\n",
      "| 50.0|0.01501|90.0| 1.21| 1.0| 0.401|7.923| 24.8| 5.885| 1.0|198.0|   13.6|395.52| 3.16|\n",
      "| 50.0|4.89822| 0.0| 18.1| 0.0| 0.631| 4.97|100.0|1.3325|24.0|666.0|   20.2|375.52| 3.26|\n",
      "| 50.0|5.66998| 0.0| 18.1| 1.0| 0.631|6.683| 96.8|1.3567|24.0|666.0|   20.2|375.33| 3.73|\n",
      "| 50.0|6.53876| 0.0| 18.1| 1.0| 0.631|7.016| 97.5|1.2024|24.0|666.0|   20.2|392.05| 2.96|\n",
      "| 50.0| 9.2323| 0.0| 18.1| 0.0| 0.631|6.216|100.0|1.1691|24.0|666.0|   20.2|366.15| 9.53|\n",
      "| 50.0|8.26725| 0.0| 18.1| 1.0| 0.668|5.875| 89.6|1.1296|24.0|666.0|   20.2|347.88| 8.88|\n",
      "+-----+-------+----+-----+----+------+-----+-----+------+----+-----+-------+------+-----+\n",
      "\n",
      "None\n",
      "16\n",
      "Number of partition before repartition: 1\n",
      "Number of partition after repartition: 4\n"
     ]
    }
   ],
   "source": [
    "print(type(data_sd))\n",
    "# printSchema prints dataframe's column names and their types. dtypes\n",
    "# contains the types of the columns.\n",
    "print(data_sd.printSchema())\n",
    "print(data_sd.dtypes)\n",
    "\n",
    "# select selects columns. filter selects rows.\n",
    "print(data_sd.select('label').show())\n",
    "print(data_sd.select(data_sd['label'], data_sd['label']+1).show())\n",
    "print(data_sd.filter(data_sd['label']>49).show())\n",
    "print(data_sd.filter(data_sd['label']>49).count())\n",
    "\n",
    "# repartition distributes the dataframe into x partitions.\n",
    "print(\"Number of partition before repartition: {}\".format(data_sd.rdd.getNumPartitions()))\n",
    "data_sd=data_sd.repartition(4)\n",
    "print(\"Number of partition after repartition: {}\".format(data_sd.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cells prepare the dataframe for ML use**\n",
    "\n",
    "The algorithms in Spark ML library need a dataframe with just two columns: the first one (typically named _features_) must contain a matrix with the input attributes, the second one must contain the output attribute (typically named _label_). In order to do that, _VectorAssembler_ is going to be used to put together all the input attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "ignore = ['label']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in data_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "data_sd = assembler.transform(data_sd).select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the first rows of the dataframe look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------+\n",
      "|label|features                                                                   |\n",
      "+-----+---------------------------------------------------------------------------+\n",
      "|21.6 |[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14]   |\n",
      "|28.7 |[0.02985,0.0,2.18,0.0,0.458,6.43,58.7,6.0622,3.0,222.0,18.7,394.12,5.21]   |\n",
      "|18.9 |[0.17004,12.5,7.87,0.0,0.524,6.004,85.9,6.5921,5.0,311.0,15.2,386.71,17.1] |\n",
      "|20.4 |[0.62976,0.0,8.14,0.0,0.538,5.949,61.8,4.7075,4.0,307.0,21.0,396.9,8.26]   |\n",
      "|17.5 |[0.7842,0.0,8.14,0.0,0.538,5.99,81.7,4.2579,4.0,307.0,21.0,386.75,14.67]   |\n",
      "|19.6 |[0.85204,0.0,8.14,0.0,0.538,5.965,89.2,4.0123,4.0,307.0,21.0,392.53,13.83] |\n",
      "|13.9 |[0.84054,0.0,8.14,0.0,0.538,5.599,85.7,4.4546,4.0,307.0,21.0,303.42,16.51] |\n",
      "|21.0 |[1.00245,0.0,8.14,0.0,0.538,6.674,87.3,4.239,4.0,307.0,21.0,380.23,11.98]  |\n",
      "|13.1 |[1.15172,0.0,8.14,0.0,0.538,5.701,95.0,3.7872,4.0,307.0,21.0,358.77,18.35] |\n",
      "|21.0 |[0.08014,0.0,5.96,0.0,0.499,5.85,41.5,3.9342,5.0,279.0,19.2,396.9,8.77]    |\n",
      "|26.6 |[0.12744,0.0,6.91,0.0,0.448,6.77,2.9,5.7209,3.0,233.0,17.9,385.41,4.84]    |\n",
      "|19.3 |[0.17142,0.0,6.91,0.0,0.448,5.682,33.8,5.1004,3.0,233.0,17.9,396.9,10.21]  |\n",
      "|19.4 |[0.21977,0.0,6.91,0.0,0.448,5.602,62.0,6.0877,3.0,233.0,17.9,396.9,16.2]   |\n",
      "|23.4 |[0.04981,21.0,5.64,0.0,0.439,5.998,21.4,6.8147,4.0,243.0,16.8,396.9,8.43]  |\n",
      "|31.6 |[0.01432,100.0,1.32,0.0,0.411,6.816,40.5,8.3248,5.0,256.0,15.1,392.9,3.95] |\n",
      "|16.0 |[0.17171,25.0,5.13,0.0,0.453,5.966,93.4,6.8185,8.0,284.0,19.7,378.08,14.44]|\n",
      "|23.5 |[0.03584,80.0,3.37,0.0,0.398,6.29,17.8,6.6115,4.0,337.0,16.1,396.9,4.67]   |\n",
      "|20.9 |[0.12816,12.5,6.07,0.0,0.409,5.885,33.0,6.498,4.0,345.0,18.9,396.9,8.79]   |\n",
      "|23.4 |[0.19539,0.0,10.81,0.0,0.413,6.245,6.2,5.2873,4.0,305.0,19.2,377.17,7.54]  |\n",
      "|20.8 |[0.08707,0.0,12.83,0.0,0.437,6.14,45.8,4.0905,5.0,398.0,18.7,386.96,10.27] |\n",
      "+-----+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sd.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next cell imports Spark decision trees for regression, and methods for evaluating regression models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are going to use the train/test strategy for evaluation.** 70% are going to be used for training, 30% for evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData_sd, testData_sd) = data_sd.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**A decision tree with default parameters is trained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "model=dt.fit(trainingData_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And predictions are computed on the test set.** The model is used to _transform_ the test dataset into a set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|        prediction|\n",
      "+-----+--------------------+------------------+\n",
      "|  7.2|[16.8118,0.0,18.1...|11.108571428571429|\n",
      "|  8.5|[7.67202,0.0,18.1...|11.108571428571429|\n",
      "| 11.5|[8.15174,0.0,18.1...|11.108571428571429|\n",
      "| 13.1|[1.15172,0.0,8.14...|15.053846153846154|\n",
      "| 13.3|[9.82349,0.0,18.1...|11.108571428571429|\n",
      "| 17.1|[9.72418,0.0,18.1...|11.108571428571429|\n",
      "| 17.7|[3.69311,0.0,18.1...|20.594166666666663|\n",
      "| 18.3|[0.26838,0.0,9.69...|20.594166666666663|\n",
      "| 18.7|[0.22212,0.0,10.0...| 20.47142857142857|\n",
      "| 18.9|[0.17004,12.5,7.8...| 20.47142857142857|\n",
      "| 19.4|[0.21977,0.0,6.91...|             17.34|\n",
      "| 20.0|[0.43571,0.0,10.5...| 20.47142857142857|\n",
      "| 20.3|[0.07165,0.0,25.6...|21.400000000000006|\n",
      "| 20.3|[0.14103,0.0,13.9...| 20.47142857142857|\n",
      "| 21.6|[0.02731,0.0,7.07...|20.594166666666663|\n",
      "| 21.6|[0.26938,0.0,9.9,...|20.594166666666663|\n",
      "| 21.7|[3.8497,0.0,18.1,...|20.594166666666663|\n",
      "| 22.0|[0.03537,34.0,6.0...|20.594166666666663|\n",
      "| 22.6|[0.06724,0.0,3.24...| 23.39565217391305|\n",
      "| 22.6|[0.13642,0.0,10.5...|20.594166666666663|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_sd = model.transform(testData_sd)\n",
    "predictions_sd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, MAE is computed on the _label_ and _prediction_ columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.22542895135\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions_sd)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "maxk = len(trainingData_sd.take(1)[0].features)\n",
    "print(maxk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using a pipeline in SPARK ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipeline 1 = pca 5 components => selector 3 features => decision tree \n",
    "pca1 = PCA(k=3, inputCol=\"features\")\n",
    "dt1 = DecisionTreeRegressor(featuresCol=pca1.getOutputCol(), \n",
    "                           labelCol=\"label\")\n",
    "\n",
    "pipeline1 = Pipeline(stages=[pca1, dt1])\n",
    "\n",
    "# Pipeline2 = selector3 => decision tree \n",
    "\n",
    "pca2 = PCA(k=5, inputCol=\"features\")\n",
    "dt2 = DecisionTreeRegressor(featuresCol=pca2.getOutputCol(), \n",
    "                           labelCol=\"label\")\n",
    "pipeline2 = Pipeline(stages=[pca2, dt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = pipeline1.fit(trainingData_sd)\n",
    "model2 = pipeline2.fit(trainingData_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions1 = model1.transform(testData_sd)\n",
    "predictions2 = model2.transform(testData_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE. 3 components vs. 5 components: 6.38704342096 vs. 5.68334521756\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae1 = evaluator.evaluate(predictions1)\n",
    "mae2 = evaluator.evaluate(predictions2)\n",
    "print(\"MAE. 3 components vs. 5 components: {} vs. {}\".format(mae1,mae2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
