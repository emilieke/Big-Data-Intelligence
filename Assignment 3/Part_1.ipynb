{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clara Caba√±as Pujadas and Emilie Krutnes Engen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD ASSIGNMENT PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming PCA in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"C:/spark-2.1.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.4-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wind\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Clara\\\\Documents'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "import os\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wind_sd = spark.read.csv(path=\"wind.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wind_sd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wind_sd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, validation and test partitions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the train, validation and test partitions\n",
    "wind_train = wind_sd.filter(wind_sd['year'] <= 2006)\n",
    "wind_val = wind_sd.filter(wind_sd['year'] > 2006).filter(wind_sd['year'] <= 2008)\n",
    "wind_test = wind_sd.filter(wind_sd['year'] > 2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the columns not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing the 5 first columns - not needed\n",
    "wind_train = wind_train.drop('steps', 'year', 'month', 'day', 'hour')\n",
    "wind_val = wind_val.drop('steps', 'year', 'month', 'day', 'hour')\n",
    "wind_test = wind_test.drop('steps', 'year', 'month', 'day', 'hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use VectorAssembler to put together the input attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "ignore = ['energy']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in wind_train.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "wind_train = assembler.transform(wind_train).select(['energy', 'features'])\n",
    "\n",
    "ignore = ['energy']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in wind_val.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "wind_val = assembler.transform(wind_val).select(['energy', 'features'])\n",
    "\n",
    "ignore = ['energy']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in wind_test.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "wind_test = assembler.transform(wind_test).select(['energy', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 469.96397500686055],\n",
       " [10, 470.60650390856256],\n",
       " [15, 474.7406058882686],\n",
       " [20, 474.84649722479975],\n",
       " [25, 474.4392560320692],\n",
       " [30, 474.4392560320692],\n",
       " [35, 447.9666423914338],\n",
       " [40, 447.84534776696665],\n",
       " [45, 446.74254591006724],\n",
       " [50, 448.43167475012575],\n",
       " [55, 448.43167475012575],\n",
       " [60, 449.695364817011],\n",
       " [65, 445.1972840388339],\n",
       " [70, 447.16127319306423],\n",
       " [75, 445.73909491083185],\n",
       " [80, 451.7444198943323],\n",
       " [85, 443.88448426773783],\n",
       " [90, 448.4568654434856],\n",
       " [95, 457.25423865382623]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline 1 = pca 5 components => selector 3 features => decision tree \n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform PCA selecting number of features between 5 and 100\n",
    "for i in range(5, 100, 5):\n",
    "\n",
    "    pca = PCA(k=i, inputCol=\"features\")\n",
    "    dt = DecisionTreeRegressor(featuresCol=pca.getOutputCol(), labelCol=\"energy\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[pca, dt])\n",
    "\n",
    "    model = pipeline.fit(wind_train)\n",
    "\n",
    "    predictions = model.transform(wind_val)\n",
    "\n",
    "    evaluator = RegressionEvaluator(\n",
    "                labelCol=\"energy\", \n",
    "                predictionCol=\"prediction\", \n",
    "                metricName=\"mae\")\n",
    "\n",
    "    mae = evaluator.evaluate(predictions)\n",
    "    \n",
    "    results.append([i, mae])\n",
    "\n",
    "results\n",
    "\n",
    "\n",
    "min(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we have that k=85 gives the lowest MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321.794387566 MAE with all the attributes.\n"
     ]
    }
   ],
   "source": [
    "k_min = 85\n",
    "\n",
    "wind_train_and_val = wind_sd.filter(wind_sd['year'] <= 2008)\n",
    "wind_train_and_val = wind_train_and_val.drop('steps', 'year', 'month', 'day', 'hour')\n",
    "\n",
    "ignore = ['energy']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in wind_train_and_val.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "wind_train_and_val = assembler.transform(wind_train_and_val).select(['energy', 'features'])\n",
    "wind_train_and_val = wind_train_and_val.withColumnRenamed('energy', 'label')\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "model=dt.fit(wind_train_and_val)\n",
    "\n",
    "predictions_sd = model.transform(wind_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "labelCol=\"energy\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions_sd)\n",
    "\n",
    "print(str(mae) +  \" MAE with all the attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
