{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clara CabaÃ±as Pujadas and Emilie Krutnes Engen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD ASSIGNMENT PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRAMMING KNN IN SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"C:/spark-2.1.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.4-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KNN\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Scikit-learn Iris dataset into a Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data            # Input attributes\n",
    "y = iris.target          # Label\n",
    "# zip is used so that each instance is a tuble of (label, input attributes). \n",
    "# This will make life easier later\n",
    "# Note: zip([1,2,3], [\"a\",\"b\",\"c\"]) => [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "data = zip(y,X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD is split into 4 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_rdd = sc.parallelize(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the rdd into a LabeledPoint RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_rdd = data_rdd.map(lambda x: LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's classify test instance p = [5.1,3.5,1.4,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = [5.3,3.2,1.6,0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You start here: (0.5 points)** Write Spark code that prints the class of the closest data_rdd instance to p. The solution is actually quite short, but you can use as many cells as you need. Try your code to be as efficient as possible. **Important:** You are not allowed to sort the RDD (i.e. you cannot use takeOrdered(), top(), sortByKey(), or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for p = [5.3,3.2,1.6,0.1] with K=1 is class: 0\n"
     ]
    }
   ],
   "source": [
    "# Get distance between point x and p\n",
    "def get_distance(x):\n",
    "    return np.linalg.norm(x-p)\n",
    "\n",
    "# Map distance between each point x and p\n",
    "mapped_rdd = data_rdd.map(lambda x: (x.label, get_distance(x.features)))\n",
    "\n",
    "# Return the class of the point closest to p\n",
    "closest = mapped_rdd.reduce(min)\n",
    "print(\"The prediction for p = [5.3,3.2,1.6,0.1] with K=1 is class: \" + str(int(closest[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may continue here: (0.5 points)** Now, write Spark code that prints the class of the **second** closest data_rdd instance to p. Again, sorting the RDD is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for p = [5.3,3.2,1.6,0.1] with K=2 is class: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Map the distances between every point x and p as long as the distance is larger than smallest distance\n",
    "mapped_rdd2 = data_rdd.map(lambda x: (x.label, get_distance(x.features)) if get_distance(x.features) > closest[1] else (x.label, float(\"inf\")))\n",
    "\n",
    "# Return the class of the second closest point\n",
    "closest2 = mapped_rdd2.reduce(min)\n",
    "\n",
    "print(\"The prediction for p = [5.3,3.2,1.6,0.1] with K=2 is class: \" + str(int(closest2[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
